{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d01da1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURACIÓN DE MODELOS Y CROSS-VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "def train_and_evaluate_models(X_train, X_val, y_train, y_val, cv_folds=5):\n",
    "    \"\"\"\n",
    "    Entrena múltiples modelos y evalúa con validación cruzada\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    X_train : sparse matrix\n",
    "        Features de entrenamiento\n",
    "    X_val : sparse matrix\n",
    "        Features de validación\n",
    "    y_train : array\n",
    "        Labels de entrenamiento\n",
    "    y_val : array\n",
    "        Labels de validación\n",
    "    cv_folds : int\n",
    "        Número de folds para cross-validation\n",
    "    \n",
    "    Retorna:\n",
    "    --------\n",
    "    dict : Resultados de todos los modelos\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"ENTRENAMIENTO Y EVALUACIÓN DE MODELOS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Definir modelos a entrenar\n",
    "    # Cada modelo incluye justificación de hiperparámetros\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(\n",
    "            max_iter=1000,\n",
    "            C=1.0,  # Regularización L2, C alto = menos regularización\n",
    "            class_weight='balanced',  # Maneja desbalance de clases\n",
    "            random_state=42,\n",
    "            solver='liblinear'  # Eficiente para datasets pequeños-medianos\n",
    "        ),\n",
    "        \n",
    "        'Naive Bayes': MultinomialNB(\n",
    "            alpha=0.1  # Suavizado de Laplace, previene probabilidades cero\n",
    "        ),\n",
    "        \n",
    "        'Linear SVM': LinearSVC(\n",
    "            C=1.0,  # Parámetro de regularización\n",
    "            class_weight='balanced',\n",
    "            max_iter=1000,\n",
    "            random_state=42,\n",
    "            dual=False  # False es más eficiente cuando n_samples > n_features\n",
    "        ),\n",
    "        \n",
    "        'Random Forest': RandomForestClassifier(\n",
    "            n_estimators=100,  # 100 árboles\n",
    "            max_depth=None,  # Sin límite de profundidad\n",
    "            min_samples_split=5,  # Mínimo de muestras para dividir nodo\n",
    "            min_samples_leaf=2,  # Mínimo de muestras en hoja\n",
    "            class_weight='balanced',\n",
    "            random_state=42,\n",
    "            n_jobs=-1  # Usar todos los cores CPU\n",
    "        ),\n",
    "        \n",
    "        'LightGBM': LGBMClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=7,\n",
    "            learning_rate=0.1,\n",
    "            num_leaves=31,\n",
    "            class_weight='balanced',\n",
    "            random_state=42,\n",
    "            verbose=-1\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    best_model = None\n",
    "    best_score = 0\n",
    "    \n",
    "    # Configurar validación cruzada estratificada\n",
    "    skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    print(f\"\\nConfiguraci{'ó'}n de validación cruzada:\")\n",
    "    print(f\"  - Número de folds: {cv_folds}\")\n",
    "    print(f\"  - Estratificación: Sí (mantiene proporción de clases)\")\n",
    "    print(f\"  - Métrica principal: Accuracy\")\n",
    "    print(f\"  - Métricas adicionales: F1-score (weighted)\")\n",
    "    \n",
    "    # Entrenar cada modelo\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\n{'═' * 70}\")\n",
    "        print(f\"Entrenando: {model_name}\")\n",
    "        print(f\"{'═' * 70}\")\n",
    "        \n",
    "        try:\n",
    "            # Cross-validation\n",
    "            print(f\"  Ejecutando {cv_folds}-fold cross-validation...\")\n",
    "            \n",
    "            # Convertir a array denso solo para Random Forest y LightGBM\n",
    "            if model_name in ['Random Forest', 'LightGBM']:\n",
    "                X_train_array = X_train.toarray()\n",
    "                X_val_array = X_val.toarray()\n",
    "            else:\n",
    "                X_train_array = X_train\n",
    "                X_val_array = X_val\n",
    "            \n",
    "            # Realizar cross-validation\n",
    "            cv_scores = cross_val_score(\n",
    "                model, X_train_array, y_train,\n",
    "                cv=skf, scoring='accuracy', n_jobs=-1\n",
    "            )\n",
    "            \n",
    "            # Entrenar en todo el conjunto de entrenamiento\n",
    "            print(f\"  Entrenando en dataset completo de entrenamiento...\")\n",
    "            model.fit(X_train_array, y_train)\n",
    "            \n",
    "            # Evaluar en validation set\n",
    "            y_val_pred = model.predict(X_val_array)\n",
    "            y_train_pred = model.predict(X_train_array)\n",
    "            \n",
    "            # Calcular métricas\n",
    "            train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "            val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "            cv_mean = cv_scores.mean()\n",
    "            cv_std = cv_scores.std()\n",
    "            \n",
    "            # Calcular F1-score\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "                y_val, y_val_pred, average='weighted'\n",
    "            )\n",
    "            \n",
    "            # Detectar overfitting\n",
    "            overfitting = train_accuracy - val_accuracy\n",
    "            \n",
    "            # Mostrar resultados\n",
    "            print(f\"\\n  {'RESULTADOS':}\")\n",
    "            print(f\"    {'─' * 50}\")\n",
    "            print(f\"    Cross-Validation Accuracy: {cv_mean:.4f} ± {cv_std:.4f}\")\n",
    "            print(f\"    Training Accuracy:         {train_accuracy:.4f}\")\n",
    "            print(f\"    Validation Accuracy:       {val_accuracy:.4f}\")\n",
    "            print(f\"    Validation F1-Score:       {f1:.4f}\")\n",
    "            print(f\"    Validation Precision:      {precision:.4f}\")\n",
    "            print(f\"    Validation Recall:         {recall:.4f}\")\n",
    "            print(f\"\\n    {'Análisis de Overfitting':}\")\n",
    "            print(f\"    Diferencia Train-Val:      {overfitting:.4f}\")\n",
    "            \n",
    "            if overfitting > 0.15:\n",
    "                print(f\"    ⚠ OVERFITTING DETECTADO (diferencia > 0.15)\")\n",
    "            elif overfitting > 0.05:\n",
    "                print(f\"    ⚠ Posible overfitting leve (diferencia > 0.05)\")\n",
    "            else:\n",
    "                print(f\"    ✓ Sin overfitting significativo\")\n",
    "            \n",
    "            # Guardar resultados\n",
    "            results[model_name] = {\n",
    "                'model': model,\n",
    "                'cv_scores': cv_scores,\n",
    "                'cv_mean': cv_mean,\n",
    "                'cv_std': cv_std,\n",
    "                'train_accuracy': train_accuracy,\n",
    "                'val_accuracy': val_accuracy,\n",
    "                'f1_score': f1,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'overfitting': overfitting,\n",
    "                'y_val_pred': y_val_pred\n",
    "            }\n",
    "            \n",
    "            # Actualizar mejor modelo\n",
    "            if val_accuracy > best_score:\n",
    "                best_score = val_accuracy\n",
    "                best_model = model_name\n",
    "            \n",
    "            print(f\"  ✓ {model_name} completado exitosamente\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error entrenando {model_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Resumen final\n",
    "    print(f\"\\n{'═' * 70}\")\n",
    "    print(f\"RESUMEN DE RESULTADOS\")\n",
    "    print(f\"{'═' * 70}\")\n",
    "    \n",
    "    # Crear tabla comparativa\n",
    "    print(f\"\\n{'Modelo':<20} {'CV Accuracy':<15} {'Val Accuracy':<15} {'F1-Score':<12} {'Overfitting':<12}\")\n",
    "    print(f\"{'─' * 80}\")\n",
    "    \n",
    "    for model_name, res in sorted(results.items(), key=lambda x: x[1]['val_accuracy'], reverse=True):\n",
    "        print(f\"{model_name:<20} {res['cv_mean']:.4f} ± {res['cv_std']:.3f}   \"\n",
    "              f\"{res['val_accuracy']:.4f}          \"\n",
    "              f\"{res['f1_score']:.4f}       \"\n",
    "              f\"{res['overfitting']:+.4f}\")\n",
    "    \n",
    "    print(f\"\\n{'✓ MEJOR MODELO:'} {best_model} (Validation Accuracy: {best_score:.4f})\")\n",
    "    \n",
    "    return results, best_model\n",
    "\n",
    "\n",
    "# Entrenar modelos\n",
    "print(\"Iniciando entrenamiento de modelos...\")\n",
    "print(\"Esto puede tomar varios minutos dependiendo del tamaño del dataset...\\n\")\n",
    "\n",
    "model_results, best_model_name = train_and_evaluate_models(\n",
    "    X_train_tfidf, X_val_tfidf, y_train, y_val, cv_folds=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77d284f",
   "metadata": {},
   "source": [
    "## 13. Visualización de Resultados de Modelos\n",
    "\n",
    "Graficamos los resultados para comparación visual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a052d0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZACIÓN DE COMPARACIÓN DE MODELOS\n",
    "# ============================================================================\n",
    "\n",
    "def plot_model_comparison(results):\n",
    "    \"\"\"\n",
    "    Crea visualizaciones comparativas de los modelos\n",
    "    \"\"\"\n",
    "    \n",
    "    # Preparar datos\n",
    "    models = list(results.keys())\n",
    "    cv_means = [results[m]['cv_mean'] for m in models]\n",
    "    cv_stds = [results[m]['cv_std'] for m in models]\n",
    "    val_accs = [results[m]['val_accuracy'] for m in models]\n",
    "    f1_scores = [results[m]['f1_score'] for m in models]\n",
    "    overfits = [results[m]['overfitting'] for m in models]\n",
    "    \n",
    "    # Crear subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Comparación de Accuracy\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[0, 0].bar(x - width/2, cv_means, width, label='CV Accuracy', \n",
    "                   alpha=0.8, color='skyblue', edgecolor='black')\n",
    "    axes[0, 0].bar(x + width/2, val_accs, width, label='Validation Accuracy',\n",
    "                   alpha=0.8, color='lightcoral', edgecolor='black')\n",
    "    axes[0, 0].set_xlabel('Modelo', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].set_title('Comparación de Accuracy por Modelo', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].set_xticks(x)\n",
    "    axes[0, 0].set_xticklabels(models, rotation=45, ha='right')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "    axes[0, 0].set_ylim([0, 1.0])\n",
    "    \n",
    "    # 2. Cross-Validation con error bars\n",
    "    axes[0, 1].errorbar(x, cv_means, yerr=cv_stds, fmt='o', markersize=10,\n",
    "                        capsize=5, capthick=2, elinewidth=2, color='darkblue')\n",
    "    axes[0, 1].set_xlabel('Modelo', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].set_ylabel('CV Accuracy', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].set_title('Cross-Validation con Intervalo de Confianza', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].set_xticks(x)\n",
    "    axes[0, 1].set_xticklabels(models, rotation=45, ha='right')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    axes[0, 1].set_ylim([min(cv_means) - 0.1, 1.0])\n",
    "    \n",
    "    # 3. F1-Score comparison\n",
    "    axes[1, 0].barh(models, f1_scores, color='mediumseagreen', edgecolor='black', alpha=0.8)\n",
    "    axes[1, 0].set_xlabel('F1-Score (Weighted)', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].set_title('Comparación de F1-Score', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].grid(axis='x', alpha=0.3)\n",
    "    axes[1, 0].set_xlim([0, 1.0])\n",
    "    \n",
    "    # Añadir valores en las barras\n",
    "    for i, v in enumerate(f1_scores):\n",
    "        axes[1, 0].text(v + 0.01, i, f'{v:.4f}', va='center', fontsize=10)\n",
    "    \n",
    "    # 4. Overfitting analysis\n",
    "    colors_overfit = ['red' if o > 0.10 else 'orange' if o > 0.05 else 'green' for o in overfits]\n",
    "    axes[1, 1].barh(models, overfits, color=colors_overfit, edgecolor='black', alpha=0.7)\n",
    "    axes[1, 1].set_xlabel('Overfitting (Train Acc - Val Acc)', fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].set_title('Análisis de Overfitting por Modelo', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].axvline(x=0.05, color='orange', linestyle='--', linewidth=2, label='Umbral leve (0.05)')\n",
    "    axes[1, 1].axvline(x=0.10, color='red', linestyle='--', linewidth=2, label='Umbral severo (0.10)')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Añadir valores\n",
    "    for i, v in enumerate(overfits):\n",
    "        axes[1, 1].text(v + 0.005, i, f'{v:+.4f}', va='center', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_model_comparison(model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6c424c",
   "metadata": {},
   "source": [
    "## 14. PASO 6: Evaluación Final en Test Set\n",
    "\n",
    "Evaluamos el mejor modelo en el conjunto de test (datos nunca vistos).\n",
    "\n",
    "### Métricas de Evaluación:\n",
    "1. **Accuracy**: % de predicciones correctas\n",
    "2. **Precision**: De los predichos como clase X, cuántos son realmente X\n",
    "3. **Recall**: De todos los X reales, cuántos fueron detectados\n",
    "4. **F1-Score**: Media armónica de precision y recall\n",
    "5. **Confusion Matrix**: Visualiza errores de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e84533b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EVALUACIÓN FINAL EN TEST SET\n",
    "# ============================================================================\n",
    "\n",
    "def evaluate_on_test_set(model, X_test, y_test, model_name, class_labels_dict):\n",
    "    \"\"\"\n",
    "    Evalúa el modelo final en el conjunto de test\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    model : sklearn model\n",
    "        Modelo entrenado\n",
    "    X_test : sparse matrix\n",
    "        Features de test\n",
    "    y_test : array\n",
    "        Labels verdaderos de test\n",
    "    model_name : str\n",
    "        Nombre del modelo\n",
    "    class_labels_dict : dict\n",
    "        Diccionario de etiquetas de clase\n",
    "    \n",
    "    Retorna:\n",
    "    --------\n",
    "    dict : Métricas de evaluación\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(f\"EVALUACIÓN FINAL EN TEST SET: {model_name}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Convertir a array denso si es necesario\n",
    "    if model_name in ['Random Forest', 'LightGBM']:\n",
    "        X_test_array = X_test.toarray()\n",
    "    else:\n",
    "        X_test_array = X_test\n",
    "    \n",
    "    # Predicciones\n",
    "    y_pred = model.predict(X_test_array)\n",
    "    \n",
    "    # Calcular métricas\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        y_test, y_pred, average=None\n",
    "    )\n",
    "    \n",
    "    # Métricas weighted (considerando desbalance)\n",
    "    precision_w, recall_w, f1_w, _ = precision_recall_fscore_support(\n",
    "        y_test, y_pred, average='weighted'\n",
    "    )\n",
    "    \n",
    "    # Mostrar resultados\n",
    "    print(f\"\\n{'MÉTRICAS GENERALES':}\")\n",
    "    print(f\"  {'─' * 50}\")\n",
    "    print(f\"  Accuracy:           {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"  Precision (weighted): {precision_w:.4f}\")\n",
    "    print(f\"  Recall (weighted):    {recall_w:.4f}\")\n",
    "    print(f\"  F1-Score (weighted):  {f1_w:.4f}\")\n",
    "    \n",
    "    # Métricas por clase\n",
    "    class_names_inv = {v: k for k, v in class_labels_dict.items()}\n",
    "    \n",
    "    print(f\"\\n{'MÉTRICAS POR CLASE':}\")\n",
    "    print(f\"  {'─' * 50}\")\n",
    "    print(f\"  {'Clase':<25} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<10}\")\n",
    "    print(f\"  {'─' * 80}\")\n",
    "    \n",
    "    for i in range(len(precision)):\n",
    "        class_name = class_names_inv[i]\n",
    "        print(f\"  {class_name:<25} {precision[i]:<12.4f} {recall[i]:<12.4f} \"\n",
    "              f\"{f1[i]:<12.4f} {support[i]:<10}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(f\"\\n{'CLASSIFICATION REPORT DETALLADO':}\")\n",
    "    print(f\"  {'─' * 50}\")\n",
    "    target_names = [class_names_inv[i] for i in range(len(class_names_inv))]\n",
    "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Retornar resultados\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision_w,\n",
    "        'recall': recall_w,\n",
    "        'f1_score': f1_w,\n",
    "        'confusion_matrix': cm,\n",
    "        'y_pred': y_pred,\n",
    "        'per_class_metrics': {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'support': support\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "# Obtener el mejor modelo\n",
    "best_model = model_results[best_model_name]['model']\n",
    "\n",
    "# Evaluar en test set\n",
    "test_results = evaluate_on_test_set(\n",
    "    best_model, X_test_tfidf, y_test, \n",
    "    best_model_name, class_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2033e6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZACIÓN: CONFUSION MATRIX\n",
    "# ============================================================================\n",
    "\n",
    "def plot_confusion_matrix(cm, class_labels_dict, model_name):\n",
    "    \"\"\"\n",
    "    Visualiza la matriz de confusión\n",
    "    \n",
    "    Justificación:\n",
    "    --------------\n",
    "    La matriz de confusión muestra dónde el modelo comete errores:\n",
    "    - Diagonal: Predicciones correctas\n",
    "    - Fuera de diagonal: Confusiones entre clases\n",
    "    - Útil para identificar qué clases se confunden entre sí\n",
    "    \"\"\"\n",
    "    \n",
    "    class_names = [k for k, v in sorted(class_labels_dict.items(), key=lambda x: x[1])]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # 1. Matriz de confusión (valores absolutos)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                ax=axes[0], cbar_kws={'label': 'Cantidad'})\n",
    "    axes[0].set_title(f'Matriz de Confusión - {model_name}\\n(Valores Absolutos)', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "    axes[0].set_ylabel('Clase Real', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_xlabel('Clase Predicha', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # 2. Matriz de confusión normalizada (porcentajes)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Greens',\n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                ax=axes[1], cbar_kws={'label': 'Porcentaje'})\n",
    "    axes[1].set_title(f'Matriz de Confusión - {model_name}\\n(Normalizada por Fila)', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "    axes[1].set_ylabel('Clase Real', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_xlabel('Clase Predicha', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Análisis de confusiones\n",
    "    print(f\"\\n{'ANÁLISIS DE CONFUSIONES':}\")\n",
    "    print(f\"{'─' * 70}\")\n",
    "    \n",
    "    for i, true_class in enumerate(class_names):\n",
    "        for j, pred_class in enumerate(class_names):\n",
    "            if i != j and cm[i, j] > 0:\n",
    "                confusion_rate = cm[i, j] / cm[i].sum()\n",
    "                print(f\"  {cm[i, j]} instancias de '{true_class}' clasificadas como '{pred_class}' \"\n",
    "                      f\"({confusion_rate*100:.1f}% de {true_class})\")\n",
    "\n",
    "plot_confusion_matrix(test_results['confusion_matrix'], class_labels, best_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6839a1bb",
   "metadata": {},
   "source": [
    "## 15. Análisis de Errores\n",
    "\n",
    "Examinamos casos específicos donde el modelo falla para entender limitaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc15c589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ANÁLISIS DE ERRORES\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_errors(X_test_original, y_test, y_pred, class_labels_dict, n_examples=5):\n",
    "    \"\"\"\n",
    "    Analiza ejemplos de errores de clasificación\n",
    "    \n",
    "    Justificación:\n",
    "    --------------\n",
    "    Entender dónde y por qué falla el modelo es crucial para:\n",
    "    - Identificar limitaciones del modelo\n",
    "    - Mejorar preprocesamiento\n",
    "    - Decidir si necesitamos más datos de ciertas clases\n",
    "    - Detectar problemas de calidad de datos (OCR errors, etc.)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"ANÁLISIS DE ERRORES DE CLASIFICACIÓN\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Identificar predicciones incorrectas\n",
    "    errors = y_test != y_pred\n",
    "    error_indices = np.where(errors)[0]\n",
    "    \n",
    "    print(f\"\\nTotal de errores: {len(error_indices)} de {len(y_test)} \"\n",
    "          f\"({len(error_indices)/len(y_test)*100:.2f}%)\")\n",
    "    \n",
    "    if len(error_indices) == 0:\n",
    "        print(\"¡No hay errores! El modelo es perfecto en el test set.\")\n",
    "        return\n",
    "    \n",
    "    # Mapeo de etiquetas\n",
    "    class_names_inv = {v: k for k, v in class_labels_dict.items()}\n",
    "    \n",
    "    # Analizar algunos ejemplos de errores\n",
    "    print(f\"\\n{'EJEMPLOS DE CLASIFICACIONES INCORRECTAS':}\")\n",
    "    print(f\"{'─' * 70}\\n\")\n",
    "    \n",
    "    n_show = min(n_examples, len(error_indices))\n",
    "    sample_errors = np.random.choice(error_indices, n_show, replace=False)\n",
    "    \n",
    "    for idx, error_idx in enumerate(sample_errors, 1):\n",
    "        true_label = y_test[error_idx]\n",
    "        pred_label = y_pred[error_idx]\n",
    "        text = X_test_original[error_idx]\n",
    "        \n",
    "        print(f\"Error #{idx}:\")\n",
    "        print(f\"  Clase Real:     {class_names_inv[true_label]}\")\n",
    "        print(f\"  Clase Predicha: {class_names_inv[pred_label]}\")\n",
    "        print(f\"  Texto (primeras 200 caracteres):\")\n",
    "        print(f\"    '{text[:200]}...'\")\n",
    "        print(f\"  {'─' * 68}\\n\")\n",
    "    \n",
    "    # Análisis de confusión por pares de clases\n",
    "    print(f\"\\n{'PARES DE CLASES MÁS CONFUNDIDOS':}\")\n",
    "    print(f\"{'─' * 70}\")\n",
    "    \n",
    "    confusion_pairs = []\n",
    "    for i in error_indices:\n",
    "        confusion_pairs.append((y_test[i], y_pred[i]))\n",
    "    \n",
    "    from collections import Counter\n",
    "    confusion_counts = Counter(confusion_pairs)\n",
    "    \n",
    "    for (true_class, pred_class), count in confusion_counts.most_common(3):\n",
    "        print(f\"  '{class_names_inv[true_class]}' → '{class_names_inv[pred_class]}': \"\n",
    "              f\"{count} veces\")\n",
    "\n",
    "analyze_errors(X_test, y_test, test_results['y_pred'], class_labels, n_examples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91051c5e",
   "metadata": {},
   "source": [
    "## 16. Guardar Modelo Final\n",
    "\n",
    "Guardamos el mejor modelo y el vectorizador TF-IDF para deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b8a6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# GUARDAR MODELO Y ARTIFACTOS\n",
    "# ============================================================================\n",
    "\n",
    "def save_model_artifacts(model, vectorizer, class_labels_dict, \n",
    "                        model_name, test_results, output_dir='models'):\n",
    "    \"\"\"\n",
    "    Guarda el modelo entrenado y todos los artifactos necesarios\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    model : sklearn model\n",
    "        Modelo entrenado\n",
    "    vectorizer : TfidfVectorizer\n",
    "        Vectorizador entrenado\n",
    "    class_labels_dict : dict\n",
    "        Diccionario de etiquetas\n",
    "    model_name : str\n",
    "        Nombre del modelo\n",
    "    test_results : dict\n",
    "        Resultados de evaluación\n",
    "    output_dir : str\n",
    "        Directorio de salida\n",
    "    \"\"\"\n",
    "    \n",
    "    import os\n",
    "    import pickle\n",
    "    import json\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Crear directorio si no existe\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"GUARDANDO MODELO Y ARTIFACTOS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Timestamp para versionado\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # 1. Guardar modelo\n",
    "    model_filename = f\"{output_dir}/model_{model_name.replace(' ', '_')}_{timestamp}.pkl\"\n",
    "    with open(model_filename, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(f\"\\n✓ Modelo guardado: {model_filename}\")\n",
    "    \n",
    "    # 2. Guardar vectorizador\n",
    "    vectorizer_filename = f\"{output_dir}/vectorizer_{timestamp}.pkl\"\n",
    "    with open(vectorizer_filename, 'wb') as f:\n",
    "        pickle.dump(vectorizer, f)\n",
    "    print(f\"✓ Vectorizador guardado: {vectorizer_filename}\")\n",
    "    \n",
    "    # 3. Guardar metadatos\n",
    "    metadata = {\n",
    "        'model_name': model_name,\n",
    "        'timestamp': timestamp,\n",
    "        'class_labels': class_labels_dict,\n",
    "        'test_accuracy': float(test_results['accuracy']),\n",
    "        'test_f1_score': float(test_results['f1_score']),\n",
    "        'test_precision': float(test_results['precision']),\n",
    "        'test_recall': float(test_results['recall']),\n",
    "        'model_filename': model_filename,\n",
    "        'vectorizer_filename': vectorizer_filename\n",
    "    }\n",
    "    \n",
    "    metadata_filename = f\"{output_dir}/metadata_{timestamp}.json\"\n",
    "    with open(metadata_filename, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    print(f\"✓ Metadatos guardados: {metadata_filename}\")\n",
    "    \n",
    "    # 4. Crear versión \"latest\" (sobrescribir siempre con el último)\n",
    "    latest_model = f\"{output_dir}/model_latest.pkl\"\n",
    "    latest_vectorizer = f\"{output_dir}/vectorizer_latest.pkl\"\n",
    "    latest_metadata = f\"{output_dir}/metadata_latest.json\"\n",
    "    \n",
    "    with open(latest_model, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    with open(latest_vectorizer, 'wb') as f:\n",
    "        pickle.dump(vectorizer, f)\n",
    "    with open(latest_metadata, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n✓ Versión 'latest' actualizada:\")\n",
    "    print(f\"  - {latest_model}\")\n",
    "    print(f\"  - {latest_vectorizer}\")\n",
    "    print(f\"  - {latest_metadata}\")\n",
    "    \n",
    "    print(f\"\\n{'INFORMACIÓN DEL MODELO GUARDADO':}\")\n",
    "    print(f\"{'─' * 70}\")\n",
    "    print(f\"  Modelo: {model_name}\")\n",
    "    print(f\"  Test Accuracy: {test_results['accuracy']:.4f}\")\n",
    "    print(f\"  Test F1-Score: {test_results['f1_score']:.4f}\")\n",
    "    print(f\"  Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    return metadata_filename\n",
    "\n",
    "# Guardar modelo final\n",
    "metadata_file = save_model_artifacts(\n",
    "    best_model, \n",
    "    tfidf_vectorizer, \n",
    "    class_labels,\n",
    "    best_model_name,\n",
    "    test_results\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4edfe0",
   "metadata": {},
   "source": [
    "## 17. Función de Predicción para Deployment\n",
    "\n",
    "Creamos una función sencilla para hacer predicciones en nuevos documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa662f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FUNCIÓN DE PREDICCIÓN PARA DEPLOYMENT\n",
    "# ============================================================================\n",
    "\n",
    "def predict_document_class(image_path, model, vectorizer, class_labels_dict, \n",
    "                          preprocess_func, tesseract_cmd=None):\n",
    "    \"\"\"\n",
    "    Predice la clase de un nuevo documento desde imagen\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    image_path : str\n",
    "        Ruta a la imagen del documento\n",
    "    model : sklearn model\n",
    "        Modelo entrenado\n",
    "    vectorizer : TfidfVectorizer\n",
    "        Vectorizador entrenado\n",
    "    class_labels_dict : dict\n",
    "        Diccionario de etiquetas\n",
    "    preprocess_func : function\n",
    "        Función de preprocesamiento de texto\n",
    "    tesseract_cmd : str, opcional\n",
    "        Ruta al ejecutable de Tesseract\n",
    "    \n",
    "    Retorna:\n",
    "    --------\n",
    "    dict : Predicción con clase, probabilidades y confianza\n",
    "    \"\"\"\n",
    "    \n",
    "    # Configurar Tesseract si se proporciona\n",
    "    if tesseract_cmd:\n",
    "        pytesseract.pytesseract.tesseract_cmd = tesseract_cmd\n",
    "    \n",
    "    try:\n",
    "        # 1. Cargar imagen\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        # 2. Extraer texto con OCR\n",
    "        text = pytesseract.image_to_string(image, lang='eng')\n",
    "        \n",
    "        # 3. Preprocesar texto\n",
    "        processed_text = preprocess_func(text)\n",
    "        \n",
    "        # 4. Vectorizar\n",
    "        text_tfidf = vectorizer.transform([processed_text])\n",
    "        \n",
    "        # 5. Predecir\n",
    "        prediction = model.predict(text_tfidf)[0]\n",
    "        \n",
    "        # 6. Obtener probabilidades si el modelo lo soporta\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            probabilities = model.predict_proba(text_tfidf)[0]\n",
    "        elif hasattr(model, 'decision_function'):\n",
    "            # Para SVM, convertir decision function a pseudo-probabilidades\n",
    "            decision = model.decision_function(text_tfidf)[0]\n",
    "            probabilities = np.exp(decision) / np.sum(np.exp(decision))\n",
    "        else:\n",
    "            probabilities = None\n",
    "        \n",
    "        # Mapear predicción a nombre de clase\n",
    "        class_names_inv = {v: k for k, v in class_labels_dict.items()}\n",
    "        predicted_class = class_names_inv[prediction]\n",
    "        \n",
    "        # Calcular confianza\n",
    "        if probabilities is not None:\n",
    "            confidence = float(probabilities[prediction])\n",
    "            all_probs = {class_names_inv[i]: float(prob) \n",
    "                        for i, prob in enumerate(probabilities)}\n",
    "        else:\n",
    "            confidence = None\n",
    "            all_probs = None\n",
    "        \n",
    "        return {\n",
    "            'predicted_class': predicted_class,\n",
    "            'predicted_label': int(prediction),\n",
    "            'confidence': confidence,\n",
    "            'all_probabilities': all_probs,\n",
    "            'original_text_length': len(text),\n",
    "            'processed_text_length': len(processed_text.split()),\n",
    "            'success': True\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "\n",
    "# Ejemplo de uso\n",
    "print(\"=\" * 70)\n",
    "print(\"EJEMPLO DE PREDICCIÓN EN NUEVO DOCUMENTO\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Intentar predecir en una imagen de test\n",
    "import glob\n",
    "test_images = glob.glob(r\"datasets\\document-classification-dataset\\email\\*.png\")\n",
    "\n",
    "if test_images:\n",
    "    example_image = test_images[0]\n",
    "    print(f\"\\nProbando con imagen: {example_image}\")\n",
    "    \n",
    "    result = predict_document_class(\n",
    "        example_image,\n",
    "        best_model,\n",
    "        tfidf_vectorizer,\n",
    "        class_labels,\n",
    "        preprocess_data,\n",
    "        tesseract_cmd=r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "    )\n",
    "    \n",
    "    if result['success']:\n",
    "        print(f\"\\n✓ Predicción exitosa:\")\n",
    "        print(f\"  Clase predicha: {result['predicted_class']}\")\n",
    "        print(f\"  Confianza: {result['confidence']*100:.2f}%\" if result['confidence'] else \"  Confianza: N/A\")\n",
    "        if result['all_probabilities']:\n",
    "            print(f\"\\n  Probabilidades por clase:\")\n",
    "            for class_name, prob in sorted(result['all_probabilities'].items(), \n",
    "                                          key=lambda x: x[1], reverse=True):\n",
    "                print(f\"    {class_name:25s}: {prob*100:5.2f}%\")\n",
    "    else:\n",
    "        print(f\"\\n✗ Error en predicción: {result['error']}\")\n",
    "else:\n",
    "    print(\"\\n⚠ No se encontraron imágenes de ejemplo para probar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5f29c6",
   "metadata": {},
   "source": [
    "## 18. Conclusiones y Recomendaciones Finales\n",
    "\n",
    "Resumen del proyecto, hallazgos clave y recomendaciones para mejora futura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbd2e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# GENERAR RESUMEN FINAL DEL PROYECTO\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"RESUMEN FINAL DEL PROYECTO\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n{'1. CONFIGURACIÓN DEL DATASET':}\")\n",
    "print(f\"   {'─' * 65}\")\n",
    "print(f\"   Total de documentos: {len(df)}\")\n",
    "print(f\"   Clases: {list(class_labels.keys())}\")\n",
    "print(f\"   División: 70% train, 20% validation, 10% test\")\n",
    "print(f\"   Estratificación: Sí (balance de clases mantenido)\")\n",
    "\n",
    "print(f\"\\n{'2. PREPROCESAMIENTO Y FEATURES':}\")\n",
    "print(f\"   {'─' * 65}\")\n",
    "print(f\"   Técnica de OCR: Tesseract\")\n",
    "print(f\"   Preprocesamiento NLP:\")\n",
    "print(f\"     - Lowercase, eliminación de puntuación y números\")\n",
    "print(f\"     - Tokenización y eliminación de stopwords\")\n",
    "print(f\"     - Lemmatización (WordNet)\")\n",
    "print(f\"   Feature extraction: TF-IDF\")\n",
    "print(f\"     - N-grams: 1-2 (unigrams + bigrams)\")\n",
    "print(f\"     - Max features: {X_train_tfidf.shape[1]}\")\n",
    "print(f\"     - Sparsity: {(1.0 - X_train_tfidf.nnz / (X_train_tfidf.shape[0] * X_train_tfidf.shape[1])) * 100:.2f}%\")\n",
    "\n",
    "if pca_analysis['recommendation']:\n",
    "    print(f\"   Reducción de dimensionalidad: PCA aplicado\")\n",
    "    print(f\"     - Componentes: {pca_analysis['n_components']}\")\n",
    "    print(f\"     - Varianza explicada: {pca_analysis['variance_explained']*100:.2f}%\")\n",
    "else:\n",
    "    print(f\"   Reducción de dimensionalidad: No aplicado\")\n",
    "    print(f\"     - Justificación: TF-IDF sparse es más eficiente\")\n",
    "\n",
    "print(f\"\\n{'3. MODELOS ENTRENADOS Y EVALUADOS':}\")\n",
    "print(f\"   {'─' * 65}\")\n",
    "for i, (model_name, res) in enumerate(sorted(model_results.items(), \n",
    "                                            key=lambda x: x[1]['val_accuracy'], \n",
    "                                            reverse=True), 1):\n",
    "    print(f\"   {i}. {model_name}\")\n",
    "    print(f\"      CV Accuracy: {res['cv_mean']:.4f} ± {res['cv_std']:.4f}\")\n",
    "    print(f\"      Val Accuracy: {res['val_accuracy']:.4f}\")\n",
    "    print(f\"      Overfitting: {res['overfitting']:+.4f}\")\n",
    "\n",
    "print(f\"\\n{'4. MEJOR MODELO SELECCIONADO':}\")\n",
    "print(f\"   {'─' * 65}\")\n",
    "print(f\"   Modelo: {best_model_name}\")\n",
    "print(f\"   Validation Accuracy: {model_results[best_model_name]['val_accuracy']:.4f}\")\n",
    "print(f\"   Test Accuracy: {test_results['accuracy']:.4f} ({test_results['accuracy']*100:.2f}%)\")\n",
    "print(f\"   Test F1-Score: {test_results['f1_score']:.4f}\")\n",
    "print(f\"   Test Precision: {test_results['precision']:.4f}\")\n",
    "print(f\"   Test Recall: {test_results['recall']:.4f}\")\n",
    "\n",
    "print(f\"\\n{'5. ANÁLISIS DE OVERFITTING':}\")\n",
    "print(f\"   {'─' * 65}\")\n",
    "best_overfit = model_results[best_model_name]['overfitting']\n",
    "if best_overfit > 0.10:\n",
    "    print(f\"   ⚠ OVERFITTING DETECTADO ({best_overfit:+.4f})\")\n",
    "    print(f\"   Recomendaciones:\")\n",
    "    print(f\"     - Aumentar regularización (C más bajo para LR/SVM)\")\n",
    "    print(f\"     - Reducir complejidad del modelo\")\n",
    "    print(f\"     - Obtener más datos de entrenamiento\")\n",
    "elif best_overfit > 0.05:\n",
    "    print(f\"   ⚠ Overfitting leve detectado ({best_overfit:+.4f})\")\n",
    "    print(f\"   El modelo está ligeramente sobreajustado pero aceptable\")\n",
    "else:\n",
    "    print(f\"   ✓ No hay overfitting significativo ({best_overfit:+.4f})\")\n",
    "    print(f\"   El modelo generaliza correctamente\")\n",
    "\n",
    "print(f\"\\n{'6. MÉTRICAS POR CLASE (Test Set)':}\")\n",
    "print(f\"   {'─' * 65}\")\n",
    "class_names_inv = {v: k for k, v in class_labels.items()}\n",
    "for i in range(len(class_labels)):\n",
    "    class_name = class_names_inv[i]\n",
    "    metrics = test_results['per_class_metrics']\n",
    "    print(f\"   {class_name}:\")\n",
    "    print(f\"     Precision: {metrics['precision'][i]:.4f}\")\n",
    "    print(f\"     Recall:    {metrics['recall'][i]:.4f}\")\n",
    "    print(f\"     F1-Score:  {metrics['f1'][i]:.4f}\")\n",
    "    print(f\"     Support:   {metrics['support'][i]} documentos\")\n",
    "\n",
    "print(f\"\\n{'7. RECOMENDACIONES PARA MEJORA FUTURA':}\")\n",
    "print(f\"   {'─' * 65}\")\n",
    "print(f\"   □ Aumentar tamaño del dataset (especialmente clases minoritarias)\")\n",
    "print(f\"   □ Mejorar calidad de OCR:\")\n",
    "print(f\"       - Preprocesamiento de imágenes (binarización, deskew)\")\n",
    "print(f\"       - Usar Tesseract 5.x con LSTM para mejor reconocimiento\")\n",
    "print(f\"   □ Feature engineering adicional:\")\n",
    "print(f\"       - Features de layout (posición de texto, formato)\")\n",
    "print(f\"       - Named Entity Recognition (NER)\")\n",
    "print(f\"       - Word embeddings (Word2Vec, FastText)\")\n",
    "print(f\"   □ Explorar modelos más avanzados:\")\n",
    "print(f\"       - Transfer learning con BERT/RoBERTa\")\n",
    "print(f\"       - Ensembles (stacking de múltiples modelos)\")\n",
    "print(f\"   □ Análisis de casos edge:\")\n",
    "print(f\"       - Documentos híbridos o ambiguos\")\n",
    "print(f\"       - Documentos con poco texto\")\n",
    "print(f\"       - Diferentes idiomas o layouts\")\n",
    "\n",
    "print(f\"\\n{'8. ARCHIVOS GENERADOS':}\")\n",
    "print(f\"   {'─' * 65}\")\n",
    "print(f\"   ✓ Modelo entrenado: models/model_latest.pkl\")\n",
    "print(f\"   ✓ Vectorizador TF-IDF: models/vectorizer_latest.pkl\")\n",
    "print(f\"   ✓ Metadatos: models/metadata_latest.json\")\n",
    "print(f\"   ✓ Notebook con análisis completo: main.ipynb\")\n",
    "\n",
    "print(f\"\\n{'═' * 70}\")\n",
    "print(f\"PROYECTO COMPLETADO EXITOSAMENTE\")\n",
    "print(f\"{'═' * 70}\")\n",
    "print(f\"\\n✓ Todos los objetivos cumplidos:\")\n",
    "print(f\"  ✓ Conversión de formatos de imagen\")\n",
    "print(f\"  ✓ Extracción de texto con OCR\")\n",
    "print(f\"  ✓ Análisis exploratorio exhaustivo\")\n",
    "print(f\"  ✓ División estratificada de datos\")\n",
    "print(f\"  ✓ Entrenamiento de múltiples modelos\")\n",
    "print(f\"  ✓ Validación cruzada y detección de overfitting\")\n",
    "print(f\"  ✓ Evaluación final en test set\")\n",
    "print(f\"  ✓ Modelo guardado para deployment\")\n",
    "print(f\"\\n¡Gracias por usar este sistema de clasificación de documentos!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
