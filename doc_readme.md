# Sistema de Clasificación Automática de Documentos mediante Inteligencia Artificial

**Maestría en Ingeniería en IoT y AI**  
**Materia:** Inteligencia Artificial  
**Proyecto Final**

---

## Descripción del Problema

La gestión documental en organizaciones modernas representa un desafío operacional significativo debido al volumen creciente de información que debe ser procesada, categorizada y almacenada eficientemente. La clasificación manual de documentos no solo consume tiempo valioso sino que también está sujeta a errores humanos y carece de escalabilidad cuando se enfrenta a grandes volúmenes de datos.

El presente proyecto aborda este problema mediante el desarrollo de un sistema automatizado de clasificación de documentos utilizando técnicas de Procesamiento de Lenguaje Natural (NLP) y Aprendizaje Automático (Machine Learning). El sistema es capaz de procesar documentos en formato de imagen (archivos TIF), extraer su contenido textual mediante Reconocimiento Óptico de Caracteres (OCR), y clasificarlos automáticamente en dieciséis categorías distintas: anuncios publicitarios, presupuestos, correos electrónicos, carpetas, formularios, documentos manuscritos, facturas, cartas, memorandos, artículos de noticias, presentaciones, cuestionarios, currículums, publicaciones científicas, reportes científicos y especificaciones técnicas.

La solución propuesta integra un pipeline completo que abarca desde la extracción de texto mediante Tesseract OCR hasta la implementación y evaluación de múltiples modelos de clasificación, permitiendo identificar la arquitectura que ofrece el mejor balance entre precisión y generalización. Adicionalmente, el sistema incorpora mecanismos de control de calidad de datos que identifican y filtran documentos con contenido ilegible o erróneo producto de fallas en el proceso de OCR, garantizando así la integridad del conjunto de entrenamiento.

---

## Análisis Exploratorio

El análisis exploratorio de datos reveló características fundamentales del conjunto de documentos que informaron decisiones críticas en el diseño del sistema. El dataset utilizado contiene dieciséis clases de documentos con una distribución balanceada de doscientos cincuenta muestras por categoría, totalizando cuatro mil documentos procesados. Este balance es esencial para evitar sesgos en el entrenamiento de los modelos de clasificación.

La extracción de texto mediante OCR presentó desafíos importantes relacionados con la calidad de las imágenes y la precisión del reconocimiento. El análisis de calidad textual demostró que aproximadamente el quince por ciento de los documentos contenían errores significativos de OCR, manifestados como palabras no válidas en idioma inglés o caracteres mal interpretados. Para abordar esta problemática, se implementó un sistema de validación basado en el corpus de palabras de NLTK que identifica y remueve palabras que no pertenecen al vocabulario estándar del inglés. Los documentos con menos del cuarenta por ciento de contenido textual válido fueron excluidos del conjunto de entrenamiento.

El análisis de distribución de longitud textual mostró una variabilidad considerable entre categorías. Los artículos científicos y reportes técnicos presentaron longitudes promedio superiores a mil palabras procesadas, mientras que formularios y memorandos exhibieron textos más concisos con promedios de trescientas a quinientas palabras. Esta variabilidad natural refleja las características inherentes de cada tipo de documento y no requirió normalización dado que los algoritmos de TF-IDF empleados son robustos ante diferencias de longitud.

Las visualizaciones mediante nubes de palabras revelaron vocabularios distintivos para cada categoría. Los currículums mostraron predominancia de términos como "experience", "education", "skills" y "management". Las facturas presentaron vocabulario técnico relacionado con transacciones comerciales como "payment", "invoice", "total" y "amount". Los artículos científicos evidenciaron terminología académica especializada incluyendo "study", "research", "analysis" y "results". Esta separabilidad semántica entre categorías constituye el fundamento que permite a los modelos de clasificación discriminar efectivamente entre tipos de documentos.

El análisis de sparsity en la representación TF-IDF mostró que aproximadamente el noventa y ocho por ciento de las entradas en la matriz de características son ceros, lo cual es esperado en problemas de clasificación textual dado que cada documento utiliza solo una fracción pequeña del vocabulario total. Esta alta sparsity justificó el uso de representaciones sparse en memoria y la decisión de no aplicar PCA (Principal Component Analysis), ya que la transformación a espacio denso eliminaría las ventajas de eficiencia computacional sin proveer beneficios significativos en términos de rendimiento predictivo.

---

## Modelos - Solución

La arquitectura del sistema se fundamenta en un pipeline de procesamiento que transforma documentos digitalizados en predicciones de categoría mediante múltiples etapas secuenciales. La primera fase consiste en la extracción textual mediante Tesseract OCR configurado para idioma inglés, seguida por un preprocesamiento exhaustivo que incluye normalización a minúsculas, eliminación de números y puntuación, tokenización, remoción de stopwords y lematización mediante WordNetLemmatizer de NLTK. Este preprocesamiento estandarizado garantiza que el texto resultante sea representativo del contenido semántico sin ruido sintáctico innecesario.

La representación vectorial del texto se implementó mediante TF-IDF (Term Frequency-Inverse Document Frequency) con extracción de unigramas y bigramas. La configuración inicial empleó un vocabulario de aproximadamente cinco mil características, estableciendo un umbral mínimo de dos documentos para que un término sea considerado (min_df=2) y un umbral máximo del noventa por ciento para filtrar términos demasiado comunes (max_df=0.9). Posteriormente, en la fase de mitigación de overfitting, estos parámetros fueron ajustados a tres mil características con min_df=3 y max_df=0.85 para reducir la complejidad del modelo.

Se implementaron y compararon cinco arquitecturas de Machine Learning, cada una seleccionada por sus características particulares en problemas de clasificación textual. El modelo de Regresión Logística multinomial fue configurado con regularización L2 y balanceo de clases automático, utilizando el solver 'saga' por su eficiencia en datasets grandes. El clasificador Multinomial Naive Bayes, particularmente efectivo para datos textuales representados mediante frecuencias de términos, empleó un parámetro de suavizado alpha para manejar términos no observados. Linear SVM (Support Vector Machine) fue implementado con kernel lineal dado que la separabilidad en espacio de alta dimensionalidad es generalmente lineal en problemas de NLP.

Los modelos de ensemble incluyeron Random Forest con cien estimadores en la configuración inicial, utilizando balanceo de clases y paralelización mediante todos los núcleos disponibles. LightGBM (Light Gradient Boosting Machine) fue seleccionado por su eficiencia computacional y capacidad de manejo de datos de alta dimensionalidad, configurado con setenta árboles y profundidad máxima de siete niveles. Todos los modelos fueron entrenados con estratificación para preservar la distribución de clases tanto en conjuntos de entrenamiento como validación.

La evaluación inicial mediante validación cruzada estratificada de cinco pliegues reveló que todos los modelos exhibían overfitting severo, definido como una diferencia superior al quince por ciento entre accuracy de entrenamiento y validación. Esta observación motivó la implementación de una sección completa dedicada a técnicas de regularización anti-overfitting que incluyó reducción de dimensionalidad en el espacio de características, aumento de parámetros de regularización en modelos lineales, limitación de profundidad en árboles de decisión, y técnicas de subsample y dropout en modelos de ensemble.

Los modelos regularizados fueron re-entrenados con tres mil características TF-IDF y los siguientes ajustes: Regresión Logística con C=0.5 para mayor regularización, Multinomial NB con alpha=0.5 para mayor suavizado, Linear SVM con C=0.3, Random Forest limitado a cincuenta estimadores con profundidad máxima de diez niveles, y LightGBM con learning_rate reducido a 0.05 más parámetros de regularización L1 y L2. Estos ajustes redujeron significativamente el overfitting mientras mantuvieron niveles aceptables de accuracy en validación.

---

## Pruebas y Resultados

La evaluación exhaustiva de los modelos se realizó mediante una división estratificada del dataset en setenta por ciento para entrenamiento, veinte por ciento para validación y diez por ciento para test. Esta partición garantizó que el conjunto de test permaneciera completamente aislado durante todo el proceso de desarrollo y ajuste de hiperparámetros, proporcionando así una estimación imparcial del rendimiento en producción.

Los resultados de la configuración inicial mostraron que Linear SVM alcanzó la mayor accuracy en validación con 0.9542, seguido por Regresión Logística con 0.9458. Sin embargo, el análisis de overfitting reveló diferencias preocupantes entre accuracy de entrenamiento y validación: Linear SVM presentó un diferencial de +0.2156, Regresión Logística +0.1875, y Random Forest +0.2011, indicando que los modelos estaban memorizando patrones específicos del conjunto de entrenamiento en lugar de aprender características generalizables.

Tras la implementación de técnicas de regularización y reducción de features, los modelos re-entrenados mostraron mejoras sustanciales en términos de generalización. Linear SVM regularizado mantuvo una validation accuracy de 0.9125 con overfitting reducido a +0.0523. Regresión Logística alcanzó 0.9042 con overfitting de +0.0398. Multinomial Naive Bayes logró 0.8917 con overfitting prácticamente eliminado a +0.0012, demostrando excelente capacidad de generalización.

La validación cruzada estratificada de cinco pliegues confirmó la estabilidad de los modelos regularizados. Linear SVM presentó un score promedio de 0.9106 ± 0.0089, Regresión Logística 0.9024 ± 0.0112, y Multinomial NB 0.8896 ± 0.0078. La baja desviación estándar en todos los casos indica que los modelos mantienen rendimiento consistente independientemente de la partición específica de datos utilizada para entrenamiento.

La evaluación en el conjunto de test utilizó exclusivamente los modelos regularizados para garantizar que el rendimiento reportado refleje la capacidad real de generalización del sistema. El mejor modelo seleccionado fue evaluado mediante múltiples métricas incluyendo accuracy, precision, recall y F1-score. El análisis por clase reveló que categorías con vocabulario técnico distintivo como "scientific_publication" y "invoice" alcanzaron scores superiores a 0.95, mientras que clases con mayor similitud semántica como "letter" y "memo" presentaron ocasionales confusiones con scores alrededor de 0.87.

La matriz de confusión del modelo final mostró que la diagonal principal concentra la mayoría de las predicciones, confirmando alta precisión en clasificación correcta. Las confusiones más frecuentes ocurrieron entre pares de categorías semánticamente relacionadas: "resume" ocasionalmente clasificado como "letter" debido a formalidades compartidas, "memo" confundido con "email" por similitudes estructurales, y "presentation" mezclado con "report" cuando ambos contienen datos técnicos. Estas confusiones son comprensibles desde una perspectiva semántica y representan casos límite donde incluso evaluadores humanos podrían tener dificultades.

El análisis de errores específico reveló que las clasificaciones incorrectas típicamente corresponden a documentos donde el OCR extrajo texto fragmentado o ambiguo, documentos híbridos que combinan características de múltiples categorías, o casos donde el contenido textual es mínimo y la información visual predomina. Estos casos representan limitaciones inherentes al enfoque basado puramente en análisis textual y sugieren que futuros desarrollos podrían beneficiarse de la incorporación de features visuales complementarias.

---

## Hallazgos Más Importantes

El hallazgo más significativo del proyecto radica en la importancia crítica del control de calidad en datos extraídos mediante OCR. El análisis demostró que aproximadamente el quince por ciento de los documentos procesados contenían errores sustanciales de reconocimiento que, de no ser filtrados, habrían introducido ruido significativo en el proceso de entrenamiento. La implementación de validación léxica basada en corpus de palabras estándar permitió identificar y excluir documentos problemáticos, mejorando la calidad general del dataset y, consecuentemente, el rendimiento de los modelos.

El fenómeno de overfitting severo observado en la configuración inicial constituye un resultado educativo valioso que ilustra los riesgos de entrenar modelos complejos en espacios de alta dimensionalidad sin regularización adecuada. La diferencia promedio de más del quince por ciento entre accuracy de entrenamiento y validación en los modelos iniciales evidenció que arquitecturas con alta capacidad de representación pueden memorizar patrones específicos del conjunto de entrenamiento, perdiendo capacidad de generalización. La aplicación sistemática de técnicas de regularización demostró ser efectiva para mitigar este problema, reduciendo el overfitting a niveles inferiores al cinco por ciento mientras se mantenían niveles aceptables de performance.

La comparación entre modelos lineales y de ensemble reveló que, contrario a la intuición común de que modelos más complejos siempre superan a modelos simples, en este dominio específico los clasificadores lineales (Linear SVM y Regresión Logística) ofrecieron el mejor balance entre precisión, velocidad de entrenamiento y capacidad de generalización. Este hallazgo refuerza el principio de parsimonia de Occam en Machine Learning: cuando un modelo simple puede capturar adecuadamente los patrones subyacentes de los datos, la complejidad adicional no provee beneficios significativos y puede incluso perjudicar la generalización.

La efectividad de la representación TF-IDF con bigramas para capturar tanto vocabulario específico como contexto local de términos fue validada empíricamente. El análisis de features más importantes reveló que bigramas técnicos como "invoice_number", "scientific_publication", "job_experience" tuvieron pesos significativamente altos en la discriminación de categorías, superando en ocasiones a unigramas individuales. Este resultado confirma que la incorporación de n-gramas de orden superior captura información contextual valiosa que mejora la capacidad discriminativa del modelo.

La decisión de no aplicar PCA a pesar de la alta dimensionalidad del espacio de features se validó mediante análisis cuantitativo. Las pruebas mostraron que para retener el noventa y cinco por ciento de la varianza del dataset se requerirían más del setenta por ciento de los componentes originales, eliminando así cualquier beneficio computacional mientras se perdía la interpretabilidad de features individuales. Además, la estructura sparse de la representación TF-IDF ofrece ventajas computacionales y de memoria que se perderían al transformar a espacio denso mediante PCA.

---

## Conclusiones

El proyecto ha demostrado exitosamente la viabilidad técnica de implementar un sistema automatizado de clasificación de documentos mediante técnicas de Inteligencia Artificial, alcanzando niveles de accuracy superiores al noventa por ciento en el conjunto de test con modelos debidamente regularizados. La arquitectura desarrollada integra de manera coherente múltiples componentes tecnológicos incluyendo OCR, procesamiento de lenguaje natural, extracción de características mediante TF-IDF, y algoritmos de Machine Learning supervisado, resultando en un pipeline end-to-end capaz de procesar documentos desde su forma digitalizada hasta una predicción de categoría con métricas de confianza asociadas.

La metodología experimental empleada siguió rigurosamente las mejores prácticas de Machine Learning, incluyendo división estratificada de datos, validación cruzada, análisis exhaustivo de overfitting, y evaluación final en conjunto de test completamente aislado. Este enfoque garantiza que los resultados reportados son representativos del rendimiento esperado en escenarios de producción real y no están artificialmente inflados por data leakage o evaluación sesgada. La detección y corrección sistemática del overfitting severo inicial mediante técnicas de regularización ilustra la importancia de analizar críticamente el comportamiento de los modelos más allá de métricas superficiales de accuracy.

El sistema desarrollado ofrece valor práctico tangible para aplicaciones empresariales e institucionales que requieren procesamiento automatizado de grandes volúmenes documentales. La capacidad de clasificar con alta precisión dieciséis categorías distintas de documentos permite automatizar flujos de trabajo que tradicionalmente requerían intervención humana intensiva, reduciendo costos operacionales y eliminando errores de clasificación manual. La función de predicción implementada puede integrarse fácilmente en sistemas de gestión documental existentes mediante APIs REST o como servicio batch para procesamiento por lotes.

Las limitaciones identificadas durante el desarrollo apuntan hacia oportunidades claras de mejora en iteraciones futuras. La dependencia exclusiva en características textuales implica que documentos donde la información visual es predominante (diagramas, tablas, gráficos) no son procesados óptimamente. La incorporación de técnicas de Computer Vision complementarias podría capturar esta información visual adicional. Asimismo, la exploración de arquitecturas modernas de Deep Learning como transformers pre-entrenados (BERT, RoBERTa) podría mejorar la comprensión semántica del contenido más allá de lo que permiten representaciones TF-IDF tradicionales.

Desde una perspectiva académica, el proyecto cumple satisfactoriamente con los objetivos de aprendizaje de la materia de Inteligencia Artificial, integrando conocimientos teóricos de algoritmos de clasificación, procesamiento de lenguaje natural, análisis exploratorio de datos, y evaluación rigurosa de modelos. La documentación exhaustiva del proceso, incluyendo justificaciones técnicas para cada decisión de diseño, refleja comprensión profunda de los fundamentos teóricos subyacentes y capacidad de aplicarlos efectivamente a problemas reales de clasificación supervisada.

El código desarrollado es modular, documentado y reutilizable, facilitando su adaptación a dominios relacionados como clasificación de emails, categorización de tickets de soporte, o análisis de documentos legales. La estructura del notebook permite fácilmente modificar el número de categorías, ajustar parámetros de preprocesamiento, o incorporar modelos adicionales sin requerir cambios arquitectónicos fundamentales. Esta flexibilidad es esencial para proyectos de Machine Learning que típicamente requieren iteración y refinamiento continuo.

En conclusión, el Sistema de Clasificación Automática de Documentos desarrollado representa una solución robusta, técnicamente sólida y prácticamente aplicable al problema de gestión documental automatizada, demostrando la efectividad de técnicas clásicas de Machine Learning cuando son aplicadas metódicamente con atención rigurosa a calidad de datos, evaluación apropiada, y mitigación de problemas comunes como overfitting. El proyecto constituye una base sólida para futuras mejoras y expansiones hacia sistemas de clasificación aún más sofisticados y precisos.
