{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dd230b4",
   "metadata": {},
   "source": [
    "# Proyecto Final: Clasificador de Documentos con IA\n",
    "## Maestría en IoT y AI - Materia de AI\n",
    "\n",
    "### Descripción del Proyecto\n",
    "Este notebook implementa un sistema de clasificación de documentos usando técnicas de NLP y Machine Learning.\n",
    "El sistema puede clasificar documentos en tres categorías: emails, resumes (CVs) y publicaciones científicas.\n",
    "\n",
    "### Pipeline del Proyecto:\n",
    "1. Conversión de formatos de imagen (TIF/PDF a PNG)\n",
    "2. Extracción de texto mediante OCR (Tesseract)\n",
    "3. Análisis exploratorio exhaustivo de datos\n",
    "4. División estratificada de datos (70% train, 20% validation, 10% test)\n",
    "5. Entrenamiento de modelos con validación cruzada\n",
    "6. Evaluación y selección del mejor modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8940b97",
   "metadata": {},
   "source": [
    "## 1. Instalación de Dependencias\n",
    "\n",
    "Instalamos las librerías necesarias para el procesamiento de imágenes, OCR, NLP y ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3170e07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LEONI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\LEONI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\LEONI\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "# Descomentar para instalar dependencias necesarias\n",
    "# %pip install nltk pytesseract scikit-learn pillow pdf2image matplotlib seaborn wordcloud xgboost lightgbm imbalanced-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af944cf8",
   "metadata": {},
   "source": [
    "## 2. Importación de Librerías\n",
    "\n",
    "Organizamos las importaciones por funcionalidad para mejor legibilidad y mantenimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9208b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPORTACIONES DE LIBRERÍAS\n",
    "# ============================================================================\n",
    "\n",
    "# --- Librerías base de Python ---\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import warnings\n",
    "from string import punctuation\n",
    "from glob import glob\n",
    "\n",
    "# --- Procesamiento de datos ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Procesamiento de imágenes ---\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "\n",
    "# --- Visualización ---\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# --- Procesamiento de lenguaje natural (NLP) ---\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# --- Machine Learning: Preprocesamiento ---\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# --- Machine Learning: Modelos ---\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# --- Machine Learning: Evaluación ---\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score,\n",
    "    precision_recall_fscore_support, roc_auc_score, roc_curve\n",
    ")\n",
    "\n",
    "# Configuración general\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✓ Librerías importadas correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a65e0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DESCARGA DE RECURSOS NLTK\n",
    "# ============================================================================\n",
    "\n",
    "# Descargamos los recursos necesarios de NLTK para procesamiento de texto\n",
    "try:\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('punkt_tab', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('omw-1.4', quiet=True)\n",
    "    print(\"✓ Recursos NLTK descargados correctamente\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Error descargando recursos NLTK: {e}\")\n",
    "\n",
    "# Cargamos la lista de stopwords en inglés\n",
    "# Estas palabras comunes serán eliminadas durante el preprocesamiento\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "print(f\"✓ Stopwords cargadas: {len(stopwords_list)} palabras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a57423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURACIÓN DE TESSERACT OCR\n",
    "# ============================================================================\n",
    "\n",
    "# Configuramos la ruta al ejecutable de Tesseract OCR\n",
    "# NOTA: Ajustar esta ruta según la instalación en tu sistema\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "# Verificamos que Tesseract esté instalado correctamente\n",
    "try:\n",
    "    version = pytesseract.get_tesseract_version()\n",
    "    print(f\"✓ Tesseract OCR versión {version} configurado correctamente\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Error: Tesseract no encontrado. Por favor instalar desde: https://github.com/UB-Mannheim/tesseract/wiki\")\n",
    "    print(f\"   Detalles del error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc1caa3",
   "metadata": {},
   "source": [
    "## 3. PASO 1a: Conversión de Archivos TIF a PNG\n",
    "\n",
    "Función para convertir imágenes en formato TIF a PNG, necesario para estandarizar el formato de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b8ffd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FUNCIONES DE CONVERSIÓN DE FORMATO DE IMAGEN\n",
    "# ============================================================================\n",
    "\n",
    "def convert_tif_to_png(input_folder, output_folder=None, delete_original=False):\n",
    "    \"\"\"\n",
    "    Convierte archivos .TIF/.TIFF a formato .PNG\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    input_folder : str\n",
    "        Ruta de la carpeta que contiene archivos TIF\n",
    "    output_folder : str, opcional\n",
    "        Ruta de la carpeta de salida. Si es None, se usa la misma carpeta\n",
    "    delete_original : bool, opcional\n",
    "        Si True, elimina los archivos TIF originales después de convertir\n",
    "    \n",
    "    Retorna:\n",
    "    --------\n",
    "    int : Número de archivos convertidos exitosamente\n",
    "    \n",
    "    Justificación:\n",
    "    --------------\n",
    "    - PNG es un formato sin pérdida de calidad y ampliamente compatible\n",
    "    - Facilita el procesamiento uniforme de todas las imágenes\n",
    "    - Reduce tamaño de archivo comparado con TIF sin comprimir\n",
    "    \"\"\"\n",
    "    \n",
    "    if output_folder is None:\n",
    "        output_folder = input_folder\n",
    "    \n",
    "    # Crear carpeta de salida si no existe\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Buscar todos los archivos TIF/TIFF\n",
    "    tif_files = glob(os.path.join(input_folder, \"*.tif\")) + \\\n",
    "                glob(os.path.join(input_folder, \"*.tiff\")) + \\\n",
    "                glob(os.path.join(input_folder, \"*.TIF\")) + \\\n",
    "                glob(os.path.join(input_folder, \"*.TIFF\"))\n",
    "    \n",
    "    converted_count = 0\n",
    "    \n",
    "    for tif_path in tif_files:\n",
    "        try:\n",
    "            # Abrir imagen TIF\n",
    "            img = Image.open(tif_path)\n",
    "            \n",
    "            # Generar nombre del archivo PNG\n",
    "            filename = os.path.basename(tif_path)\n",
    "            png_filename = os.path.splitext(filename)[0] + '.png'\n",
    "            png_path = os.path.join(output_folder, png_filename)\n",
    "            \n",
    "            # Convertir y guardar como PNG\n",
    "            img.save(png_path, 'PNG')\n",
    "            converted_count += 1\n",
    "            \n",
    "            # Eliminar original si se solicita\n",
    "            if delete_original:\n",
    "                os.remove(tif_path)\n",
    "            \n",
    "            print(f\"  ✓ Convertido: {filename} → {png_filename}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error convirtiendo {filename}: {e}\")\n",
    "    \n",
    "    print(f\"\\n✓ Conversión completada: {converted_count} archivos convertidos\")\n",
    "    return converted_count\n",
    "\n",
    "\n",
    "# Ejemplo de uso (comentado - descomentar para usar):\n",
    "# convert_tif_to_png(r\"datasets\\mi_carpeta_tif\", r\"datasets\\mi_carpeta_png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0537c7de",
   "metadata": {},
   "source": [
    "## 4. PASO 1b: Conversión de Archivos PDF a PNG\n",
    "\n",
    "Función para convertir documentos PDF a imágenes PNG, extrayendo cada página como imagen individual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08222aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pdf_to_png(input_folder, output_folder=None, dpi=200):\n",
    "    \"\"\"\n",
    "    Convierte archivos PDF a imágenes PNG (una imagen por página)\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    input_folder : str\n",
    "        Ruta de la carpeta que contiene archivos PDF\n",
    "    output_folder : str, opcional\n",
    "        Ruta de la carpeta de salida. Si es None, se crea subcarpeta 'png_output'\n",
    "    dpi : int, opcional\n",
    "        Resolución de la imagen de salida (default: 200)\n",
    "        Mayor DPI = mejor calidad pero archivos más grandes\n",
    "    \n",
    "    Retorna:\n",
    "    --------\n",
    "    int : Número total de páginas convertidas\n",
    "    \n",
    "    Justificación del DPI:\n",
    "    ----------------------\n",
    "    - 200 DPI: Balance óptimo entre calidad OCR y tamaño de archivo\n",
    "    - Tesseract OCR funciona eficientemente entre 150-300 DPI\n",
    "    - DPI muy alto (>300) aumenta tiempo de procesamiento sin mejora significativa\n",
    "    \n",
    "    Nota:\n",
    "    -----\n",
    "    Requiere instalar: pip install pdf2image\n",
    "    En Windows también requiere: poppler (descargar desde https://github.com/oschwartz10612/poppler-windows/releases/)\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        from pdf2image import convert_from_path\n",
    "    except ImportError:\n",
    "        print(\"⚠ Error: pdf2image no está instalado.\")\n",
    "        print(\"  Instalar con: pip install pdf2image\")\n",
    "        print(\"  En Windows también necesitas poppler: https://github.com/oschwartz10612/poppler-windows/releases/\")\n",
    "        return 0\n",
    "    \n",
    "    if output_folder is None:\n",
    "        output_folder = os.path.join(input_folder, \"png_output\")\n",
    "    \n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Buscar todos los archivos PDF\n",
    "    pdf_files = glob(os.path.join(input_folder, \"*.pdf\")) + \\\n",
    "                glob(os.path.join(input_folder, \"*.PDF\"))\n",
    "    \n",
    "    total_pages = 0\n",
    "    \n",
    "    for pdf_path in pdf_files:\n",
    "        try:\n",
    "            # Convertir PDF a lista de imágenes\n",
    "            images = convert_from_path(pdf_path, dpi=dpi)\n",
    "            \n",
    "            filename_base = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "            \n",
    "            # Guardar cada página como PNG\n",
    "            for i, image in enumerate(images, start=1):\n",
    "                png_filename = f\"{filename_base}_page_{i:03d}.png\"\n",
    "                png_path = os.path.join(output_folder, png_filename)\n",
    "                image.save(png_path, 'PNG')\n",
    "                total_pages += 1\n",
    "            \n",
    "            print(f\"  ✓ Convertido: {os.path.basename(pdf_path)} ({len(images)} páginas)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error convirtiendo {os.path.basename(pdf_path)}: {e}\")\n",
    "    \n",
    "    print(f\"\\n✓ Conversión completada: {total_pages} páginas convertidas\")\n",
    "    return total_pages\n",
    "\n",
    "\n",
    "# Ejemplo de uso (comentado - descomentar para usar):\n",
    "# convert_pdf_to_png(r\"datasets\\mi_carpeta_pdf\", r\"datasets\\mi_carpeta_png\", dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95114b78",
   "metadata": {},
   "source": [
    "## 5. PASO 2: Preprocesamiento de Texto\n",
    "\n",
    "Función que limpia y normaliza el texto extraído por OCR para mejorar la calidad de los features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd48d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FUNCIÓN DE PREPROCESAMIENTO DE TEXTO\n",
    "# ============================================================================\n",
    "\n",
    "def preprocess_data(text):\n",
    "    \"\"\"\n",
    "    Preprocesa el texto extraído por OCR aplicando técnicas de NLP\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    text : str\n",
    "        Texto crudo extraído por OCR\n",
    "    \n",
    "    Retorna:\n",
    "    --------\n",
    "    str : Texto preprocesado y normalizado\n",
    "    \n",
    "    Justificación de cada paso:\n",
    "    ----------------------------\n",
    "    1. Lowercase: Normaliza el texto, reduce dimensionalidad del vocabulario\n",
    "    2. Eliminación de saltos de línea y tabulaciones: Limpia formato OCR\n",
    "    3. Eliminación de espacios múltiples: Normaliza espaciado\n",
    "    4. Eliminación de números: Los números tienen poco valor semántico para clasificación\n",
    "       de tipo de documento (el contenido conceptual importa más que valores específicos)\n",
    "    5. Eliminación de puntuación: Reduce ruido, la estructura sintáctica es menos relevante\n",
    "       que el vocabulario para este problema\n",
    "    6. Tokenización: Divide texto en palabras individuales\n",
    "    7. Eliminación de stopwords: Elimina palabras comunes sin valor discriminativo\n",
    "       (the, is, at, which, on, etc.)\n",
    "    8. Lemmatización: Reduce palabras a su forma base (running → run, better → good)\n",
    "       - Preferimos lemmatización sobre stemming porque preserva palabras reales\n",
    "       - Stemming sería más agresivo pero puede generar tokens sin significado\n",
    "    \n",
    "    Nota sobre la elección de técnicas:\n",
    "    -----------------------------------\n",
    "    - Para clasificación de documentos, el vocabulario técnico y específico de dominio\n",
    "      es más importante que la estructura gramatical\n",
    "    - La lemmatización preserva el significado mientras reduce variabilidad\n",
    "    - Este preprocesamiento es estándar para problemas de clasificación de texto\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Convertir a minúsculas\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Eliminar saltos de línea y tabulaciones\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "    \n",
    "    # 3. Eliminar espacios múltiples\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    \n",
    "    # 4. Eliminar números\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # 5. Eliminar puntuación y caracteres especiales\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # 6. Tokenización: dividir texto en palabras\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # 7. Eliminar puntuación residual y stopwords\n",
    "    data = [token for token in tokens if token not in punctuation]\n",
    "    data = [token for token in data if token not in stopwords_list]\n",
    "    \n",
    "    # 8. Lemmatización: reducir palabras a su forma base\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    final_text = []\n",
    "    for token in data:\n",
    "        word = lemmatizer.lemmatize(token)\n",
    "        final_text.append(word)\n",
    "    \n",
    "    # Retornar texto procesado como string\n",
    "    return \" \".join(final_text)\n",
    "\n",
    "\n",
    "# Probar la función con texto de ejemplo\n",
    "example_text = \"This is an EXAMPLE text with numbers 123 and punctuation!!!\"\n",
    "print(\"Texto original:\", example_text)\n",
    "print(\"Texto procesado:\", preprocess_data(example_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5227ec5",
   "metadata": {},
   "source": [
    "## 6. PASO 2 (continuación): Extracción de Texto desde Imágenes\n",
    "\n",
    "Función que procesa una carpeta de imágenes, extrae texto con OCR y crea un DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff87a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FUNCIÓN DE CARGA Y EXTRACCIÓN DE TEXTO\n",
    "# ============================================================================\n",
    "\n",
    "def load_documents_from_images(dataset_path, class_labels_dict):\n",
    "    \"\"\"\n",
    "    Carga imágenes de documentos, extrae texto con OCR y crea DataFrame\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    dataset_path : str\n",
    "        Ruta a la carpeta principal del dataset\n",
    "        Estructura esperada: dataset_path/clase1/imagen1.png\n",
    "                            dataset_path/clase2/imagen1.png\n",
    "    class_labels_dict : dict\n",
    "        Diccionario mapeando nombre de clase a número {nombre: id}\n",
    "        Ejemplo: {'email': 0, 'resume': 1, 'scientific_publication': 2}\n",
    "    \n",
    "    Retorna:\n",
    "    --------\n",
    "    pd.DataFrame : DataFrame con columnas ['Text', 'Label', 'Label_Name', 'Filename']\n",
    "    \n",
    "    Justificación de estructura:\n",
    "    ----------------------------\n",
    "    - Organizar imágenes por carpetas facilita el etiquetado automático\n",
    "    - Incluir filename permite trazabilidad y debugging\n",
    "    - Guardar tanto label numérico como nombre facilita análisis\n",
    "    \n",
    "    Manejo de errores:\n",
    "    ------------------\n",
    "    - Si una imagen falla en OCR, se registra pero no detiene el proceso\n",
    "    - Esto hace el pipeline robusto ante imágenes corruptas o ilegibles\n",
    "    \"\"\"\n",
    "    \n",
    "    final_text = []\n",
    "    final_label = []\n",
    "    final_label_name = []\n",
    "    final_filename = []\n",
    "    \n",
    "    # Obtener lista de carpetas (clases)\n",
    "    image_folders = [f for f in os.listdir(dataset_path) \n",
    "                     if os.path.isdir(os.path.join(dataset_path, f))]\n",
    "    \n",
    "    print(f\"Procesando {len(image_folders)} clases de documentos...\")\n",
    "    \n",
    "    for label_name in image_folders:\n",
    "        # Verificar que la clase esté en el diccionario\n",
    "        if label_name not in class_labels_dict:\n",
    "            print(f\"  ⚠ Advertencia: '{label_name}' no está en class_labels_dict, omitiendo...\")\n",
    "            continue\n",
    "        \n",
    "        label_path = os.path.join(dataset_path, label_name)\n",
    "        image_files = [f for f in os.listdir(label_path) \n",
    "                       if f.lower().endswith(('.png', '.jpg', '.jpeg', '.tif', '.tiff'))]\n",
    "        \n",
    "        print(f\"\\n  Procesando clase '{label_name}': {len(image_files)} imágenes\")\n",
    "        \n",
    "        for idx, filename in enumerate(image_files, 1):\n",
    "            try:\n",
    "                # Cargar imagen\n",
    "                image_path = os.path.join(label_path, filename)\n",
    "                image = Image.open(image_path)\n",
    "                \n",
    "                # Extraer texto con OCR\n",
    "                text = pytesseract.image_to_string(image, lang='eng')\n",
    "                \n",
    "                # Preprocesar texto\n",
    "                text_data = preprocess_data(text)\n",
    "                \n",
    "                # Validar que el texto no esté vacío\n",
    "                if len(text_data.strip()) == 0:\n",
    "                    print(f\"    ⚠ Advertencia: {filename} generó texto vacío después de preprocesamiento\")\n",
    "                    continue\n",
    "                \n",
    "                # Almacenar datos\n",
    "                final_text.append(text_data)\n",
    "                final_label.append(class_labels_dict[label_name])\n",
    "                final_label_name.append(label_name)\n",
    "                final_filename.append(filename)\n",
    "                \n",
    "                if idx % 20 == 0:\n",
    "                    print(f\"    Progreso: {idx}/{len(image_files)} imágenes procesadas\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    ✗ Error procesando {filename}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    # Crear DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'Text': final_text,\n",
    "        'Label': final_label,\n",
    "        'Label_Name': final_label_name,\n",
    "        'Filename': final_filename\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n✓ Extracción completada: {len(df)} documentos procesados exitosamente\")\n",
    "    print(f\"✓ Distribución de clases:\")\n",
    "    print(df['Label_Name'].value_counts())\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Ejemplo de uso (comentado - se ejecutará más adelante)\n",
    "# class_labels = {'email': 0, 'resume': 1, 'scientific_publication': 2}\n",
    "# df = load_documents_from_images(r\"datasets\\document-classification-dataset\", class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6afacb",
   "metadata": {},
   "source": [
    "## 7. Carga del Dataset Principal\n",
    "\n",
    "Ejecutamos la carga de datos del dataset principal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846b470c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CARGA DEL DATASET\n",
    "# ============================================================================\n",
    "\n",
    "# Definir las clases y sus etiquetas numéricas\n",
    "# Estas son las categorías de documentos que vamos a clasificar\n",
    "class_labels = {\n",
    "    'email': 0,\n",
    "    'resume': 1,\n",
    "    'scientific_publication': 2\n",
    "}\n",
    "\n",
    "# Ruta al dataset principal\n",
    "DATASET_PATH = r\"datasets\\document-classification-dataset\"\n",
    "\n",
    "# Cargar y procesar el dataset\n",
    "print(\"Iniciando carga del dataset...\")\n",
    "print(\"=\" * 70)\n",
    "df = load_documents_from_images(DATASET_PATH, class_labels)\n",
    "\n",
    "# Mostrar información básica del dataset\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"INFORMACIÓN DEL DATASET CARGADO\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total de documentos: {len(df)}\")\n",
    "print(f\"Columnas: {list(df.columns)}\")\n",
    "print(f\"\\nPrimeras filas del dataset:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc4a46e",
   "metadata": {},
   "source": [
    "## 8. PASO 3: Análisis Exploratorio Exhaustivo de Datos (EDA)\n",
    "\n",
    "El análisis exploratorio es crucial para entender las características del dataset y tomar decisiones informadas sobre el modelado.\n",
    "\n",
    "### Objetivos del EDA:\n",
    "1. Verificar balance/desbalance de clases\n",
    "2. Analizar distribución de longitud de textos\n",
    "3. Identificar vocabulario más frecuente por clase\n",
    "4. Detectar posibles problemas de calidad de datos\n",
    "5. Visualizar características discriminativas entre clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb04b916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ANÁLISIS EXPLORATORIO: Información General del Dataset\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_dataset_overview(df):\n",
    "    \"\"\"\n",
    "    Muestra estadísticas generales del dataset\n",
    "    \n",
    "    Justificación:\n",
    "    --------------\n",
    "    - Entender el tamaño del dataset nos ayuda a elegir modelos apropiados\n",
    "    - Dataset pequeño (<1000): modelos simples como Logistic Regression, Naive Bayes\n",
    "    - Dataset grande (>10000): podemos usar modelos más complejos como ensemble methods\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"ANÁLISIS EXPLORATORIO DE DATOS (EDA)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\n1. DIMENSIONES DEL DATASET\")\n",
    "    print(f\"   {'─' * 50}\")\n",
    "    print(f\"   Total de documentos: {len(df)}\")\n",
    "    print(f\"   Total de features: {df.shape[1]}\")\n",
    "    print(f\"   Memoria utilizada: {df.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
    "    \n",
    "    print(f\"\\n2. TIPOS DE DATOS\")\n",
    "    print(f\"   {'─' * 50}\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    print(f\"\\n3. VALORES FALTANTES\")\n",
    "    print(f\"   {'─' * 50}\")\n",
    "    missing = df.isnull().sum()\n",
    "    if missing.sum() == 0:\n",
    "        print(\"   ✓ No hay valores faltantes\")\n",
    "    else:\n",
    "        print(missing[missing > 0])\n",
    "    \n",
    "    print(f\"\\n4. ESTADÍSTICAS DE LONGITUD DE TEXTO\")\n",
    "    print(f\"   {'─' * 50}\")\n",
    "    df['text_length'] = df['Text'].apply(lambda x: len(x.split()))\n",
    "    \n",
    "    stats = df['text_length'].describe()\n",
    "    print(f\"   Promedio de palabras por documento: {stats['mean']:.2f}\")\n",
    "    print(f\"   Mediana: {stats['50%']:.2f}\")\n",
    "    print(f\"   Mínimo: {stats['min']:.0f} palabras\")\n",
    "    print(f\"   Máximo: {stats['max']:.0f} palabras\")\n",
    "    print(f\"   Desviación estándar: {stats['std']:.2f}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Ejecutar análisis general\n",
    "df = analyze_dataset_overview(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0813c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ANÁLISIS EXPLORATORIO: Distribución de Clases\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_class_distribution(df):\n",
    "    \"\"\"\n",
    "    Analiza el balance de clases en el dataset\n",
    "    \n",
    "    Justificación:\n",
    "    --------------\n",
    "    - Dataset balanceado: accuracy es una buena métrica\n",
    "    - Dataset desbalanceado: necesitamos usar F1-score, precision, recall\n",
    "    - Desbalance severo (>10:1): considerar técnicas de resampling (SMOTE, undersampling)\n",
    "    \n",
    "    Criterio de balance:\n",
    "    --------------------\n",
    "    - Balanceado: diferencia <20% entre clases\n",
    "    - Ligeramente desbalanceado: 20-50%\n",
    "    - Moderadamente desbalanceado: 50-100%\n",
    "    - Severamente desbalanceado: >100%\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n5. DISTRIBUCIÓN DE CLASES\")\n",
    "    print(f\"   {'─' * 50}\")\n",
    "    \n",
    "    class_counts = df['Label_Name'].value_counts()\n",
    "    print(f\"\\n   Conteo absoluto:\")\n",
    "    for class_name, count in class_counts.items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"   {class_name:25s}: {count:4d} documentos ({percentage:5.2f}%)\")\n",
    "    \n",
    "    # Calcular ratio de desbalance\n",
    "    max_count = class_counts.max()\n",
    "    min_count = class_counts.min()\n",
    "    imbalance_ratio = max_count / min_count\n",
    "    \n",
    "    print(f\"\\n   Ratio de desbalance: {imbalance_ratio:.2f}:1\")\n",
    "    \n",
    "    if imbalance_ratio < 1.2:\n",
    "        balance_status = \"✓ Dataset BALANCEADO\"\n",
    "        recommendation = \"No se requieren técnicas de balanceo\"\n",
    "    elif imbalance_ratio < 1.5:\n",
    "        balance_status = \"⚠ Dataset LIGERAMENTE DESBALANCEADO\"\n",
    "        recommendation = \"Considerar usar class_weight='balanced' en modelos\"\n",
    "    elif imbalance_ratio < 2.0:\n",
    "        balance_status = \"⚠ Dataset MODERADAMENTE DESBALANCEADO\"\n",
    "        recommendation = \"Usar class_weight='balanced' y métricas como F1-score\"\n",
    "    else:\n",
    "        balance_status = \"✗ Dataset SEVERAMENTE DESBALANCEADO\"\n",
    "        recommendation = \"Considerar SMOTE, undersampling o estratified sampling\"\n",
    "    \n",
    "    print(f\"\\n   Estado: {balance_status}\")\n",
    "    print(f\"   Recomendación: {recommendation}\")\n",
    "    \n",
    "    return class_counts, imbalance_ratio\n",
    "\n",
    "class_counts, imbalance_ratio = analyze_class_distribution(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bf82fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZACIÓN: Distribución de Clases\n",
    "# ============================================================================\n",
    "\n",
    "def plot_class_distribution(df):\n",
    "    \"\"\"\n",
    "    Crea visualizaciones de la distribución de clases\n",
    "    \n",
    "    Justificación:\n",
    "    --------------\n",
    "    - Las visualizaciones facilitan identificar desbalances\n",
    "    - Gráfico de barras: mejor para comparar cantidades exactas\n",
    "    - Gráfico de pastel: mejor para ver proporciones relativas\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Gráfico de barras\n",
    "    class_counts = df['Label_Name'].value_counts()\n",
    "    colors = sns.color_palette(\"husl\", len(class_counts))\n",
    "    \n",
    "    axes[0].bar(class_counts.index, class_counts.values, color=colors, edgecolor='black', alpha=0.7)\n",
    "    axes[0].set_xlabel('Clase de Documento', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_ylabel('Cantidad de Documentos', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_title('Distribución de Clases - Gráfico de Barras', fontsize=14, fontweight='bold')\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Añadir valores en las barras\n",
    "    for i, (class_name, count) in enumerate(class_counts.items()):\n",
    "        axes[0].text(i, count + max(class_counts) * 0.02, str(count), \n",
    "                     ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # Gráfico de pastel\n",
    "    axes[1].pie(class_counts.values, labels=class_counts.index, autopct='%1.1f%%',\n",
    "                colors=colors, startangle=90, explode=[0.05] * len(class_counts))\n",
    "    axes[1].set_title('Distribución de Clases - Proporción', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_class_distribution(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ca7c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZACIÓN: Distribución de Longitud de Texto por Clase\n",
    "# ============================================================================\n",
    "\n",
    "def plot_text_length_distribution(df):\n",
    "    \"\"\"\n",
    "    Analiza la distribución de longitud de texto por clase\n",
    "    \n",
    "    Justificación:\n",
    "    --------------\n",
    "    - Si las clases tienen longitudes características diferentes, esto es un feature útil\n",
    "    - Por ejemplo: emails tienden a ser más cortos que publicaciones científicas\n",
    "    - Esta información puede ser usada como feature adicional en el modelo\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # 1. Boxplot de longitud por clase\n",
    "    df.boxplot(column='text_length', by='Label_Name', ax=axes[0, 0])\n",
    "    axes[0, 0].set_title('Distribución de Longitud de Texto por Clase', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Clase de Documento', fontsize=11)\n",
    "    axes[0, 0].set_ylabel('Número de Palabras', fontsize=11)\n",
    "    plt.sca(axes[0, 0])\n",
    "    plt.xticks(rotation=15)\n",
    "    \n",
    "    # 2. Histograma superpuesto\n",
    "    for label_name in df['Label_Name'].unique():\n",
    "        subset = df[df['Label_Name'] == label_name]['text_length']\n",
    "        axes[0, 1].hist(subset, alpha=0.5, label=label_name, bins=20, edgecolor='black')\n",
    "    \n",
    "    axes[0, 1].set_xlabel('Número de Palabras', fontsize=11)\n",
    "    axes[0, 1].set_ylabel('Frecuencia', fontsize=11)\n",
    "    axes[0, 1].set_title('Histograma de Longitud de Texto', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(alpha=0.3)\n",
    "    \n",
    "    # 3. Violin plot\n",
    "    import seaborn as sns\n",
    "    sns.violinplot(data=df, x='Label_Name', y='text_length', ax=axes[1, 0])\n",
    "    axes[1, 0].set_title('Violin Plot - Distribución de Longitud', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Clase de Documento', fontsize=11)\n",
    "    axes[1, 0].set_ylabel('Número de Palabras', fontsize=11)\n",
    "    axes[1, 0].tick_params(axis='x', rotation=15)\n",
    "    \n",
    "    # 4. Estadísticas por clase\n",
    "    stats_by_class = df.groupby('Label_Name')['text_length'].describe()[['mean', 'std', 'min', 'max']]\n",
    "    \n",
    "    axes[1, 1].axis('off')\n",
    "    table_data = []\n",
    "    for idx, row in stats_by_class.iterrows():\n",
    "        table_data.append([idx, f\"{row['mean']:.1f}\", f\"{row['std']:.1f}\", \n",
    "                          f\"{row['min']:.0f}\", f\"{row['max']:.0f}\"])\n",
    "    \n",
    "    table = axes[1, 1].table(cellText=table_data, \n",
    "                             colLabels=['Clase', 'Media', 'Std', 'Min', 'Max'],\n",
    "                             cellLoc='center', loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1, 2)\n",
    "    axes[1, 1].set_title('Estadísticas de Longitud por Clase', fontsize=12, fontweight='bold', pad=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nANÁLISIS DE LONGITUD DE TEXTO POR CLASE:\")\n",
    "    print(\"─\" * 70)\n",
    "    for label_name in df['Label_Name'].unique():\n",
    "        subset = df[df['Label_Name'] == label_name]['text_length']\n",
    "        print(f\"\\n{label_name}:\")\n",
    "        print(f\"  Promedio: {subset.mean():.2f} palabras\")\n",
    "        print(f\"  Rango: {subset.min():.0f} - {subset.max():.0f} palabras\")\n",
    "\n",
    "plot_text_length_distribution(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e313fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ANÁLISIS: Vocabulario y Palabras Más Frecuentes\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_vocabulary(df, top_n=20):\n",
    "    \"\"\"\n",
    "    Analiza el vocabulario más frecuente por clase\n",
    "    \n",
    "    Justificación:\n",
    "    --------------\n",
    "    - Identificar palabras discriminativas ayuda a entender qué aprenderá el modelo\n",
    "    - Si hay mucho solapamiento de vocabulario, el problema es más difícil\n",
    "    - Palabras únicas por clase son buenos indicadores de esa clase\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    top_n : int\n",
    "        Número de palabras más frecuentes a mostrar por clase\n",
    "    \"\"\"\n",
    "    \n",
    "    from collections import Counter\n",
    "    \n",
    "    print(f\"\\nANÁLISIS DE VOCABULARIO (Top {top_n} palabras por clase)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    vocab_by_class = {}\n",
    "    \n",
    "    for label_name in df['Label_Name'].unique():\n",
    "        # Obtener todos los textos de esta clase\n",
    "        class_texts = df[df['Label_Name'] == label_name]['Text']\n",
    "        \n",
    "        # Combinar todos los textos y contar palabras\n",
    "        all_words = []\n",
    "        for text in class_texts:\n",
    "            all_words.extend(text.split())\n",
    "        \n",
    "        # Contar frecuencias\n",
    "        word_counts = Counter(all_words)\n",
    "        vocab_by_class[label_name] = word_counts\n",
    "        \n",
    "        # Mostrar top palabras\n",
    "        print(f\"\\n{label_name.upper()}:\")\n",
    "        print(f\"{'─' * 50}\")\n",
    "        print(f\"  Vocabulario total: {len(word_counts)} palabras únicas\")\n",
    "        print(f\"  Total de palabras: {sum(word_counts.values())}\")\n",
    "        print(f\"\\n  Top {top_n} palabras más frecuentes:\")\n",
    "        \n",
    "        for i, (word, count) in enumerate(word_counts.most_common(top_n), 1):\n",
    "            print(f\"    {i:2d}. {word:20s}: {count:4d} veces\")\n",
    "    \n",
    "    return vocab_by_class\n",
    "\n",
    "vocab_by_class = analyze_vocabulary(df, top_n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8f9ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZACIÓN: Word Clouds por Clase\n",
    "# ============================================================================\n",
    "\n",
    "def plot_wordclouds(df):\n",
    "    \"\"\"\n",
    "    Genera word clouds para cada clase de documento\n",
    "    \n",
    "    Justificación:\n",
    "    --------------\n",
    "    - Word clouds permiten visualizar rápidamente el vocabulario característico\n",
    "    - Palabras grandes = más frecuentes en esa clase\n",
    "    - Ayuda a validar que el OCR y preprocesamiento funcionan correctamente\n",
    "    \"\"\"\n",
    "    \n",
    "    classes = df['Label_Name'].unique()\n",
    "    n_classes = len(classes)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, n_classes, figsize=(18, 5))\n",
    "    \n",
    "    if n_classes == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, label_name in enumerate(classes):\n",
    "        # Obtener todos los textos de esta clase\n",
    "        class_texts = ' '.join(df[df['Label_Name'] == label_name]['Text'])\n",
    "        \n",
    "        # Generar word cloud\n",
    "        wordcloud = WordCloud(width=800, height=400, \n",
    "                             background_color='white',\n",
    "                             colormap='viridis',\n",
    "                             max_words=100,\n",
    "                             relative_scaling=0.5,\n",
    "                             min_font_size=10).generate(class_texts)\n",
    "        \n",
    "        # Mostrar\n",
    "        axes[idx].imshow(wordcloud, interpolation='bilinear')\n",
    "        axes[idx].set_title(f'Word Cloud: {label_name}', fontsize=14, fontweight='bold')\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_wordclouds(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9e1e1e",
   "metadata": {},
   "source": [
    "## 9. PASO 4: División Estratificada de Datos (70-20-10)\n",
    "\n",
    "División del dataset en conjuntos de entrenamiento, validación y prueba.\n",
    "\n",
    "### Justificación de la División 70-20-10:\n",
    "- **70% Entrenamiento**: Suficiente datos para que el modelo aprenda patrones\n",
    "- **20% Validación**: Para ajustar hiperparámetros y evitar overfitting\n",
    "- **10% Test**: Evaluación final del modelo con datos nunca vistos\n",
    "\n",
    "### Estratificación:\n",
    "- Mantenemos la proporción de clases en cada conjunto\n",
    "- Crítico para datasets desbalanceados\n",
    "- Asegura representatividad en train/val/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295651c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DIVISIÓN ESTRATIFICADA DEL DATASET\n",
    "# ============================================================================\n",
    "\n",
    "def split_dataset_stratified(df, train_size=0.7, val_size=0.2, test_size=0.1, random_state=42):\n",
    "    \"\"\"\n",
    "    Divide el dataset en train, validation y test de forma estratificada\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Dataset completo\n",
    "    train_size : float\n",
    "        Proporción para entrenamiento (default: 0.7)\n",
    "    val_size : float\n",
    "        Proporción para validación (default: 0.2)\n",
    "    test_size : float\n",
    "        Proporción para test (default: 0.1)\n",
    "    random_state : int\n",
    "        Semilla para reproducibilidad\n",
    "    \n",
    "    Retorna:\n",
    "    --------\n",
    "    tuple : (X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "    \n",
    "    Justificación:\n",
    "    --------------\n",
    "    - Estratificación mantiene proporciones de clases en todos los conjuntos\n",
    "    - Random state asegura reproducibilidad de experimentos\n",
    "    - División en 3 conjuntos permite validación adecuada sin contaminar test set\n",
    "    \n",
    "    Proceso:\n",
    "    --------\n",
    "    1. Primero dividimos en train+val (90%) y test (10%)\n",
    "    2. Luego dividimos train+val en train (70%) y val (20%)\n",
    "    3. Esto garantiza las proporciones 70-20-10 del total\n",
    "    \"\"\"\n",
    "    \n",
    "    # Verificar que las proporciones sumen 1.0\n",
    "    assert abs(train_size + val_size + test_size - 1.0) < 0.001, \\\n",
    "        f\"Las proporciones deben sumar 1.0 (actual: {train_size + val_size + test_size})\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"DIVISIÓN ESTRATIFICADA DEL DATASET\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Extraer features (X) y labels (y)\n",
    "    X = df['Text'].values\n",
    "    y = df['Label'].values\n",
    "    y_names = df['Label_Name'].values\n",
    "    \n",
    "    # Paso 1: Separar test set (10%)\n",
    "    test_proportion = test_size\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=test_proportion,\n",
    "        stratify=y,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Paso 2: Separar train y validation del resto\n",
    "    # val_size_adjusted es la proporción de validación respecto al conjunto temporal\n",
    "    val_size_adjusted = val_size / (train_size + val_size)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp,\n",
    "        test_size=val_size_adjusted,\n",
    "        stratify=y_temp,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Mostrar estadísticas\n",
    "    print(f\"\\nTamaño total del dataset: {len(df)} documentos\")\n",
    "    print(f\"\\n{'Conjunto':<15} {'Documentos':>12} {'Proporción':>12}\")\n",
    "    print(\"─\" * 45)\n",
    "    print(f\"{'Entrenamiento':<15} {len(X_train):>12} {len(X_train)/len(df)*100:>11.1f}%\")\n",
    "    print(f\"{'Validación':<15} {len(X_val):>12} {len(X_val)/len(df)*100:>11.1f}%\")\n",
    "    print(f\"{'Test':<15} {len(X_test):>12} {len(X_test)/len(df)*100:>11.1f}%\")\n",
    "    \n",
    "    # Verificar estratificación\n",
    "    print(f\"\\n{'Distribución de clases en cada conjunto:':}\")\n",
    "    print(\"─\" * 70)\n",
    "    \n",
    "    class_names = {v: k for k, v in class_labels.items()}\n",
    "    \n",
    "    for split_name, y_split in [('Train', y_train), ('Validation', y_val), ('Test', y_test)]:\n",
    "        print(f\"\\n{split_name}:\")\n",
    "        unique, counts = np.unique(y_split, return_counts=True)\n",
    "        for label, count in zip(unique, counts):\n",
    "            percentage = (count / len(y_split)) * 100\n",
    "            print(f\"  {class_names[label]:25s}: {count:3d} ({percentage:5.2f}%)\")\n",
    "    \n",
    "    print(\"\\n✓ División completada exitosamente\")\n",
    "    print(\"✓ Estratificación verificada: proporciones de clases mantenidas\")\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "# Ejecutar división del dataset\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_dataset_stratified(\n",
    "    df, \n",
    "    train_size=0.7, \n",
    "    val_size=0.2, \n",
    "    test_size=0.1,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c6973a",
   "metadata": {},
   "source": [
    "## 10. Feature Engineering: TF-IDF Vectorization\n",
    "\n",
    "Convertimos el texto a representación numérica usando TF-IDF.\n",
    "\n",
    "### Justificación de TF-IDF:\n",
    "- **TF (Term Frequency)**: Mide qué tan frecuente es una palabra en un documento\n",
    "- **IDF (Inverse Document Frequency)**: Penaliza palabras que aparecen en muchos documentos\n",
    "- **TF-IDF = TF × IDF**: Resalta palabras importantes pero no comunes\n",
    "\n",
    "### Por qué TF-IDF para este problema:\n",
    "1. **Eficaz para clasificación de texto**: Captura importancia relativa de palabras\n",
    "2. **Reduce peso de palabras comunes**: Automáticamente maneja palabras frecuentes\n",
    "3. **Sparse pero eficiente**: Matrices sparse ahorran memoria\n",
    "4. **Baseline sólido**: Estado del arte para muchos problemas de NLP\n",
    "\n",
    "### Alternativas consideradas:\n",
    "- **Bag of Words (Count)**: Más simple pero ignora importancia relativa\n",
    "- **Word Embeddings (Word2Vec, GloVe)**: Capturan semántica pero requieren más datos\n",
    "- **BERT/Transformers**: Mejor rendimiento pero computacionalmente costoso\n",
    "\n",
    "### Configuración de TF-IDF:\n",
    "- **ngram_range=(1,2)**: Incluye palabras individuales y bigramas\n",
    "  - Unigrama: \"machine learning\" → [\"machine\", \"learning\"]\n",
    "  - Bigrama: \"machine learning\" → [\"machine\", \"learning\", \"machine learning\"]\n",
    "  - Bigramas capturan contexto local y frases específicas de dominio\n",
    "- **max_features**: Limitamos dimensionalidad para evitar overfitting\n",
    "- **min_df**: Ignoramos palabras muy raras (ruido)\n",
    "- **max_df**: Ignoramos palabras muy comunes (poco discriminativas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb990ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FEATURE ENGINEERING: TF-IDF VECTORIZATION\n",
    "# ============================================================================\n",
    "\n",
    "def create_tfidf_features(X_train, X_val, X_test, \n",
    "                          ngram_range=(1, 2), \n",
    "                          max_features=5000,\n",
    "                          min_df=2,\n",
    "                          max_df=0.95):\n",
    "    \"\"\"\n",
    "    Convierte texto a features TF-IDF\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    X_train, X_val, X_test : array-like\n",
    "        Conjuntos de texto\n",
    "    ngram_range : tuple\n",
    "        Rango de n-gramas a considerar (default: unigrams + bigrams)\n",
    "    max_features : int\n",
    "        Número máximo de features a extraer\n",
    "    min_df : int o float\n",
    "        Frecuencia mínima de documento (ignora términos muy raros)\n",
    "    max_df : float\n",
    "        Frecuencia máxima de documento (ignora términos muy comunes)\n",
    "    \n",
    "    Retorna:\n",
    "    --------\n",
    "    tuple : (X_train_tfidf, X_val_tfidf, X_test_tfidf, vectorizer)\n",
    "    \n",
    "    Nota importante:\n",
    "    ----------------\n",
    "    - SOLO entrenamos el vectorizer con X_train (fit)\n",
    "    - Aplicamos la transformación a val y test (transform)\n",
    "    - Esto previene data leakage del conjunto de test\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"FEATURE ENGINEERING: TF-IDF VECTORIZATION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Crear vectorizador TF-IDF\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        ngram_range=ngram_range,\n",
    "        max_features=max_features,\n",
    "        min_df=min_df,\n",
    "        max_df=max_df,\n",
    "        sublinear_tf=True,  # Usa escala logarítmica para TF\n",
    "        use_idf=True\n",
    "    )\n",
    "    \n",
    "    # Entrenar SOLO con datos de entrenamiento\n",
    "    print(f\"\\nEntrenando TF-IDF vectorizer...\")\n",
    "    print(f\"  Configuración:\")\n",
    "    print(f\"    - N-gram range: {ngram_range}\")\n",
    "    print(f\"    - Max features: {max_features}\")\n",
    "    print(f\"    - Min document frequency: {min_df}\")\n",
    "    print(f\"    - Max document frequency: {max_df}\")\n",
    "    \n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "    \n",
    "    # Transformar validation y test\n",
    "    X_val_tfidf = vectorizer.transform(X_val)\n",
    "    X_test_tfidf = vectorizer.transform(X_test)\n",
    "    \n",
    "    # Mostrar estadísticas\n",
    "    print(f\"\\n✓ Vectorización completada\")\n",
    "    print(f\"\\n  Estadísticas de features:\")\n",
    "    print(f\"    - Vocabulario total: {len(vectorizer.vocabulary_)} términos\")\n",
    "    print(f\"    - Features generados: {X_train_tfidf.shape[1]}\")\n",
    "    print(f\"\\n  Dimensiones de matrices:\")\n",
    "    print(f\"    - Train:      {X_train_tfidf.shape} (documentos × features)\")\n",
    "    print(f\"    - Validation: {X_val_tfidf.shape}\")\n",
    "    print(f\"    - Test:       {X_test_tfidf.shape}\")\n",
    "    print(f\"\\n  Sparsity (% de valores cero):\")\n",
    "    print(f\"    - Train:      {(1.0 - X_train_tfidf.nnz / (X_train_tfidf.shape[0] * X_train_tfidf.shape[1])) * 100:.2f}%\")\n",
    "    \n",
    "    # Mostrar algunos features importantes\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    print(f\"\\n  Ejemplos de features extraídos:\")\n",
    "    print(f\"    Primeros 10: {list(feature_names[:10])}\")\n",
    "    print(f\"    Últimos 10:  {list(feature_names[-10:])}\")\n",
    "    \n",
    "    return X_train_tfidf, X_val_tfidf, X_test_tfidf, vectorizer\n",
    "\n",
    "\n",
    "# Crear features TF-IDF\n",
    "X_train_tfidf, X_val_tfidf, X_test_tfidf, tfidf_vectorizer = create_tfidf_features(\n",
    "    X_train, X_val, X_test,\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=5000,\n",
    "    min_df=2,\n",
    "    max_df=0.95\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad304913",
   "metadata": {},
   "source": [
    "## 11. Análisis de PCA (Principal Component Analysis)\n",
    "\n",
    "Evaluamos si PCA es apropiado para este problema.\n",
    "\n",
    "### ¿Qué es PCA?\n",
    "- Técnica de reducción de dimensionalidad\n",
    "- Encuentra direcciones de máxima varianza en los datos\n",
    "- Proyecta datos a espacio de menor dimensión\n",
    "\n",
    "### Criterios para aplicar PCA:\n",
    "1. **Alta dimensionalidad**: TF-IDF genera muchas features (5000+)\n",
    "2. **Features correlacionados**: PCA es útil si hay redundancia\n",
    "3. **Reducir overfitting**: Menos features = menor riesgo de overfitting\n",
    "4. **Visualización**: PCA permite visualizar datos en 2D/3D\n",
    "\n",
    "### Desventajas de PCA para NLP:\n",
    "1. **Pérdida de interpretabilidad**: Componentes principales no son palabras\n",
    "2. **TF-IDF ya es sparse**: PCA genera matrices densas (más memoria)\n",
    "3. **Puede perder información discriminativa**: Features raros pero importantes\n",
    "\n",
    "### Decisión:\n",
    "Evaluaremos si PCA mejora el rendimiento, pero anticipamos que para clasificación\n",
    "de documentos con TF-IDF, mantener features originales suele ser mejor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bd635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ANÁLISIS DE PCA: ¿ES NECESARIO?\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_pca_necessity(X_train_tfidf, y_train, n_components=0.95):\n",
    "    \"\"\"\n",
    "    Analiza si PCA es beneficioso para este problema\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    X_train_tfidf : sparse matrix\n",
    "        Features TF-IDF de entrenamiento\n",
    "    y_train : array\n",
    "        Labels de entrenamiento\n",
    "    n_components : float o int\n",
    "        Si float (0-1): mantiene ese % de varianza\n",
    "        Si int: mantiene ese número de componentes\n",
    "    \n",
    "    Retorna:\n",
    "    --------\n",
    "    dict : Estadísticas de PCA para toma de decisión\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"ANÁLISIS DE PCA (Principal Component Analysis)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\nDimensionalidad actual: {X_train_tfidf.shape[1]} features\")\n",
    "    print(f\"Sparsity: {(1.0 - X_train_tfidf.nnz / (X_train_tfidf.shape[0] * X_train_tfidf.shape[1])) * 100:.2f}%\")\n",
    "    \n",
    "    # Convertir a array denso para PCA (solo para análisis)\n",
    "    print(f\"\\nConvirtiendo matriz sparse a densa para análisis PCA...\")\n",
    "    print(f\"⚠ Advertencia: Esto puede consumir mucha memoria\")\n",
    "    \n",
    "    try:\n",
    "        X_dense = X_train_tfidf.toarray()\n",
    "        \n",
    "        # Aplicar PCA\n",
    "        pca = PCA(n_components=n_components, random_state=42)\n",
    "        X_pca = pca.fit_transform(X_dense)\n",
    "        \n",
    "        # Estadísticas\n",
    "        print(f\"\\n✓ PCA completado\")\n",
    "        print(f\"\\nResultados:\")\n",
    "        print(f\"  Componentes mantenidos: {pca.n_components_}\")\n",
    "        print(f\"  Varianza explicada: {pca.explained_variance_ratio_.sum()*100:.2f}%\")\n",
    "        print(f\"  Reducción de dimensionalidad: {X_train_tfidf.shape[1]} → {pca.n_components_}\")\n",
    "        print(f\"  Factor de reducción: {X_train_tfidf.shape[1] / pca.n_components_:.2f}x\")\n",
    "        \n",
    "        # Varianza por componente\n",
    "        print(f\"\\n  Varianza explicada por los primeros 10 componentes:\")\n",
    "        for i in range(min(10, len(pca.explained_variance_ratio_))):\n",
    "            print(f\"    PC{i+1}: {pca.explained_variance_ratio_[i]*100:.2f}%\")\n",
    "        \n",
    "        # Visualizar varianza acumulada\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "        plt.plot(range(1, len(cumsum)+1), cumsum, 'bo-', linewidth=2, markersize=4)\n",
    "        plt.xlabel('Número de Componentes', fontsize=12)\n",
    "        plt.ylabel('Varianza Explicada Acumulada', fontsize=12)\n",
    "        plt.title('Curva de Varianza Acumulada de PCA', fontsize=14, fontweight='bold')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.axhline(y=0.95, color='r', linestyle='--', label='95% varianza')\n",
    "        plt.axhline(y=0.90, color='orange', linestyle='--', label='90% varianza')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Decisión sobre PCA\n",
    "        print(f\"\\n{'DECISIÓN SOBRE EL USO DE PCA:':}\")\n",
    "        print(\"─\" * 70)\n",
    "        \n",
    "        if pca.n_components_ < X_train_tfidf.shape[1] * 0.3:\n",
    "            print(f\"✓ RECOMENDACIÓN: USAR PCA\")\n",
    "            print(f\"  Razón: Reducción significativa ({X_train_tfidf.shape[1] / pca.n_components_:.1f}x) manteniendo 95% varianza\")\n",
    "            recommendation = True\n",
    "        else:\n",
    "            print(f\"✗ RECOMENDACIÓN: NO USAR PCA\")\n",
    "            print(f\"  Razón: Se requieren {pca.n_components_} componentes (>{X_train_tfidf.shape[1] * 0.3:.0f})\")\n",
    "            print(f\"  - La reducción no es suficientemente significativa\")\n",
    "            print(f\"  - TF-IDF sparse es más eficiente en memoria\")\n",
    "            print(f\"  - Mantenemos interpretabilidad de features\")\n",
    "            recommendation = False\n",
    "        \n",
    "        return {\n",
    "            'pca': pca,\n",
    "            'n_components': pca.n_components_,\n",
    "            'variance_explained': pca.explained_variance_ratio_.sum(),\n",
    "            'X_pca': X_pca,\n",
    "            'recommendation': recommendation\n",
    "        }\n",
    "        \n",
    "    except MemoryError:\n",
    "        print(f\"\\n✗ Error: Memoria insuficiente para PCA completo\")\n",
    "        print(f\"\\nDECISIÓN: NO USAR PCA\")\n",
    "        print(f\"  - Dataset demasiado grande para PCA\")\n",
    "        print(f\"  - Continuaremos con TF-IDF sparse\")\n",
    "        return {'recommendation': False}\n",
    "\n",
    "\n",
    "# Analizar PCA\n",
    "pca_analysis = analyze_pca_necessity(X_train_tfidf, y_train, n_components=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a11b8e6",
   "metadata": {},
   "source": [
    "## 12. PASO 5: Entrenamiento de Modelos con Validación Cruzada\n",
    "\n",
    "Entrenaremos múltiples algoritmos y seleccionaremos el mejor mediante validación cruzada.\n",
    "\n",
    "### Algoritmos Seleccionados y Justificación:\n",
    "\n",
    "#### 1. **Logistic Regression**\n",
    "- **Pros**: Rápido, interpretable, funciona bien con features sparse\n",
    "- **Cons**: Asume separación lineal\n",
    "- **Uso**: Baseline excelente para clasificación de texto\n",
    "\n",
    "#### 2. **Multinomial Naive Bayes**\n",
    "- **Pros**: Diseñado específicamente para datos de conteo (TF-IDF)\n",
    "- **Pros**: Muy rápido, funciona bien con poco datos\n",
    "- **Cons**: Asume independencia de features (raramente cierto)\n",
    "- **Uso**: Estado del arte clásico para clasificación de texto\n",
    "\n",
    "#### 3. **Linear SVM (LinearSVC)**\n",
    "- **Pros**: Encuentra hiperplano de máxima separación, robusto\n",
    "- **Pros**: Eficiente con datos high-dimensional sparse\n",
    "- **Cons**: Sensible a escala, requiere tuning de C\n",
    "- **Uso**: Muy efectivo para clasificación de documentos\n",
    "\n",
    "#### 4. **Random Forest**\n",
    "- **Pros**: Maneja relaciones no lineales, robusto a overfitting\n",
    "- **Pros**: Importancia de features interpretable\n",
    "- **Cons**: Puede ser lento con muchos features, no optimizado para sparse\n",
    "- **Uso**: Ensemble method robusto\n",
    "\n",
    "#### 5. **Gradient Boosting (LightGBM)**\n",
    "- **Pros**: Estado del arte para muchos problemas, maneja no-linealidad\n",
    "- **Pros**: LightGBM es rápido y eficiente con memoria\n",
    "- **Cons**: Requiere tuning cuidadoso, riesgo de overfitting\n",
    "- **Uso**: Potencialmente el mejor rendimiento\n",
    "\n",
    "### Validación Cruzada Estratificada (5-fold):\n",
    "- Divide datos en 5 partes\n",
    "- Entrena 5 veces, cada vez usando 4 partes para train y 1 para validation\n",
    "- Promedia resultados para estimación robusta\n",
    "- **Detecta overfitting**: Si training score >> validation score → overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307e8c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURACIÓN DE MODELOS Y CROSS-VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "def train_and_evaluate_models(X_train, X_val, y_train, y_val, cv_folds=5):\n",
    "    \"\"\"\n",
    "    Entrena múltiples modelos y evalúa con validación cruzada\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    X_train : sparse matrix\n",
    "        Features de entrenamiento\n",
    "    X_val : sparse matrix\n",
    "        Features de validación\n",
    "    y_train : array\n",
    "        Labels de entrenamiento\n",
    "    y_val : array\n",
    "        Labels de validación\n",
    "    cv_folds : int\n",
    "        Número de folds para cross-validation\n",
    "    \n",
    "    Retorna:\n",
    "    --------\n",
    "    dict : Resultados de todos los modelos\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"ENTRENAMIENTO Y EVALUACIÓN DE MODELOS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Definir modelos a entrenar\n",
    "    # Cada modelo incluye justificación de hiperparámetros\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(\n",
    "            max_iter=1000,\n",
    "            C=1.0,  # Regularización L2, C alto = menos regularización\n",
    "            class_weight='balanced',  # Maneja desbalance de clases\n",
    "            random_state=42,\n",
    "            solver='liblinear'  # Eficiente para datasets pequeños-medianos\n",
    "        ),\n",
    "        \n",
    "        'Naive Bayes': MultinomialNB(\n",
    "            alpha=0.1  # Suavizado de Laplace, previene probabilidades cero\n",
    "        ),\n",
    "        \n",
    "        'Linear SVM': LinearSVC(\n",
    "            C=1.0,  # Parámetro de regularización\n",
    "            class_weight='balanced',\n",
    "            max_iter=1000,\n",
    "            random_state=42,\n",
    "            dual=False  # False es más eficiente cuando n_samples > n_features\n",
    "        ),\n",
    "        \n",
    "        'Random Forest': RandomForestClassifier(\n",
    "            n_estimators=100,  # 100 árboles\n",
    "            max_depth=None,  # Sin límite de profundidad\n",
    "            min_samples_split=5,  # Mínimo de muestras para dividir nodo\n",
    "            min_samples_leaf=2,  # Mínimo de muestras en hoja\n",
    "            class_weight='balanced',\n",
    "            random_state=42,\n",
    "            n_jobs=-1  # Usar todos los cores CPU\n",
    "        ),\n",
    "        \n",
    "        'LightGBM': LGBMClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=7,\n",
    "            learning_rate=0.1,\n",
    "            num_leaves=31,\n",
    "            class_weight='balanced',\n",
    "            random_state=42,\n",
    "            verbose=-1\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    best_model = None\n",
    "    best_score = 0\n",
    "    \n",
    "    # Configurar validación cruzada estratificada\n",
    "    skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    print(f\"\\nConfiguración de validación cruzada:\")\n",
    "    print(f\"  - Número de folds: {cv_folds}\")\n",
    "    print(f\"  - Estratificación: Sí (mantiene proporción de clases)\")\n",
    "    print(f\"  - Métrica principal: Accuracy\")\n",
    "    print(f\"  - Métricas adicionales: F1-score (weighted)\")\n",
    "    \n",
    "    # Entrenar cada modelo\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\n{'═' * 70}\")\n",
    "        print(f\"Entrenando: {model_name}\")\n",
    "        print(f\"{'═' * 70}\")\n",
    "        \n",
    "        try:\n",
    "            # Cross-validation\n",
    "            print(f\"  Ejecutando {cv_folds}-fold cross-validation...\")\n",
    "            \n",
    "            # Convertir a array denso solo para Random Forest y LightGBM\n",
    "            if model_name in ['Random Forest', 'LightGBM']:\n",
    "                X_train_array = X_train.toarray()\n",
    "                X_val_array = X_val.toarray()\n",
    "            else:\n",
    "                X_train_array = X_train\n",
    "                X_val_array = X_val\n",
    "            \n",
    "            # Realizar cross-validation\n",
    "            cv_scores = cross_val_score(\n",
    "                model, X_train_array, y_train,\n",
    "                cv=skf, scoring='accuracy', n_jobs=-1\n",
    "            )\n",
    "            \n",
    "            # Entrenar en todo el conjunto de entrenamiento\n",
    "            print(f\"  Entrenando en dataset completo de entrenamiento...\")\n",
    "            model.fit(X_train_array, y_train)\n",
    "            \n",
    "            # Evaluar en validation set\n",
    "            y_val_pred = model.predict(X_val_array)\n",
    "            y_train_pred = model.predict(X_train_array)\n",
    "            \n",
    "            # Calcular métricas\n",
    "            train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "            val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "            cv_mean = cv_scores.mean()\n",
    "            cv_std = cv_scores.std()\n",
    "            \n",
    "            # Calcular F1-score\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "                y_val, y_val_pred, average='weighted'\n",
    "            )\n",
    "            \n",
    "            # Detectar overfitting\n",
    "            overfitting = train_accuracy - val_accuracy\n",
    "            \n",
    "            # Mostrar resultados\n",
    "            print(f\"\\n  {'RESULTADOS':}\")\n",
    "            print(f\"    {'─' * 50}\")\n",
    "            print(f\"    Cross-Validation Accuracy: {cv_mean:.4f} ± {cv_std:.4f}\")\n",
    "            print(f\"    Training Accuracy:         {train_accuracy:.4f}\")\n",
    "            print(f\"    Validation Accuracy:       {val_accuracy:.4f}\")\n",
    "            print(f\"    Validation F1-Score:       {f1:.4f}\")\n",
    "            print(f\"    Validation Precision:      {precision:.4f}\")\n",
    "            print(f\"    Validation Recall:         {recall:.4f}\")\n",
    "            print(f\"\\n    {'Análisis de Overfitting':}\")\n",
    "            print(f\"    Diferencia Train-Val:      {overfitting:.4f}\")\n",
    "            \n",
    "            if overfitting > 0.15:\n",
    "                print(f\"    ⚠ OVERFITTING DETECTADO (diferencia > 0.15)\")\n",
    "            elif overfitting > 0.05:\n",
    "                print(f\"    ⚠ Posible overfitting leve (diferencia > 0.05)\")\n",
    "            else:\n",
    "                print(f\"    ✓ Sin overfitting significativo\")\n",
    "            \n",
    "            # Guardar resultados\n",
    "            results[model_name] = {\n",
    "                'model': model,\n",
    "                'cv_scores': cv_scores,\n",
    "                'cv_mean': cv_mean,\n",
    "                'cv_std': cv_std,\n",
    "                'train_accuracy': train_accuracy,\n",
    "                'val_accuracy': val_accuracy,\n",
    "                'f1_score': f1,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'overfitting': overfitting,\n",
    "                'y_val_pred': y_val_pred\n",
    "            }\n",
    "            \n",
    "            # Actualizar mejor modelo\n",
    "            if val_accuracy > best_score:\n",
    "                best_score = val_accuracy\n",
    "                best_model = model_name\n",
    "            \n",
    "            print(f\"  ✓ {model_name} completado exitosamente\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error entrenando {model_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Resumen final\n",
    "    print(f\"\\n{'═' * 70}\")\n",
    "    print(f\"RESUMEN DE RESULTADOS\")\n",
    "    print(f\"{'═' * 70}\")\n",
    "    \n",
    "    # Crear tabla comparativa\n",
    "    print(f\"\\n{'Modelo':<20} {'CV Accuracy':<15} {'Val Accuracy':<15} {'F1-Score':<12} {'Overfitting':<12}\")\n",
    "    print(f\"{'─' * 80}\")\n",
    "    \n",
    "    for model_name, res in sorted(results.items(), key=lambda x: x[1]['val_accuracy'], reverse=True):\n",
    "        print(f\"{model_name:<20} {res['cv_mean']:.4f} ± {res['cv_std']:.3f}   \"\n",
    "              f\"{res['val_accuracy']:.4f}          \"\n",
    "              f\"{res['f1_score']:.4f}       \"\n",
    "              f\"{res['overfitting']:+.4f}\")\n",
    "    \n",
    "    print(f\"\\n{'✓ MEJOR MODELO:'} {best_model} (Validation Accuracy: {best_score:.4f})\")\n",
    "    \n",
    "    return results, best_model\n",
    "\n",
    "\n",
    "# Entrenar modelos\n",
    "print(\"Iniciando entrenamiento de modelos...\")\n",
    "print(\"Esto puede tomar varios minutos dependiendo del tamaño del dataset...\\n\")\n",
    "\n",
    "model_results, best_model_name = train_and_evaluate_models(\n",
    "    X_train_tfidf, X_val_tfidf, y_train, y_val, cv_folds=5\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IoTDocker1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
